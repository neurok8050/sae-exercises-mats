{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVzj-5vnOJYQ"
   },
   "source": [
    "# **[Exercises]** Toy Models of Superposition & Sparse AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzpGGRLH1tc8"
   },
   "source": [
    "Colab: [exercises](https://colab.research.google.com/drive/15S4ISFVMQtfc0FPi29HRaX03dWxL65zx?usp=sharing) | [solutions](https://colab.research.google.com/drive/19Qo9wj5rGLjb6KsB9NkKNJkMiHcQhLqo?usp=sharing)\n",
    "\n",
    "> *This is a version of the Toy Models of Superposition exercises which has been curated for the use of Neel Nanda's SERI MATS scholars, and others who are interested in SAEs but not necessarily the rest of the ARENA curriculum. An adapted version of this will be included in the upcoming ARENA curriculum (possibly split into two separate sets of exercises).*\n",
    "\n",
    "Please email me at `cal.s.mcdougall@gmail.com` if there are any issues (or [join our Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-1uvoagohe-JUv9xB7Vr143pdx1UBPrzQ) and message me there!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST7GZ0xkxW6j"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/galaxies.jpeg\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd3LpCav3UXu"
   },
   "source": [
    "# Introduction & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgLu9a6v5DSw"
   },
   "source": [
    "Superposition is a crucially important concept for understanding how transformers work. A definition from Neel Nanda's glossary:\n",
    "\n",
    "> Superposition is when a model represents more than n features in an $n$-dimensional activation space. That is, features still correspond to directions, but **the set of interpretable directions is larger than the number of dimensions**.\n",
    "\n",
    "Why should we expect something like this to happen? In general, the world has way more features than the model has dimensions of freedom, and so we can't have a one-to-one mapping between features and values in our model. But the model has to represent these features somehow. Hence, it comes up with techniques for cramming multiple features into fewer dimensions (at the cost of adding noise and interference between features).\n",
    "\n",
    "The exercises here are split into three main sections (described in more detail below). The first three sections introduce Anthropic's toy models of superposition, and show how this model can illustrate important concepts about superposition. The next two sections dive deeper into specific lines of research that grew from the toy model setup. The last two sections introduce **sparse autoencoders**, an exciting line of research which hopes to solve the barriers to interpretability which superposition presents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGdc4BlI5DSw"
   },
   "source": [
    "## Content & Learning Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdzGTym25DSw"
   },
   "source": [
    "Unlike many other topics in this chapter, there's quite a bit of theory which needs to be understood before we start making inferences from the results of our coding experiments. We start by suggesting a few useful papers / videos / blog posts for you to go through, then we'll move into replication of the main results from the \"toy models\" paper. We'll conclude with a discussion of future directions for superposition research.\n",
    "\n",
    "A key point to make here is that, perhaps more so than any other section in this chapter, we really don't understand superposition that well at all! It's hard to point to the seminal work in this field because we don't really know what the critical new insights will look like. That being said, we hope this material gives you enough directions to pursue when you're finished!\n",
    "\n",
    "This notebook is split into 7 chapters. It's easily the longest single set of exercises in this entire chapter (possibly even more than twice as long as any other set if you count all exercises), however we've kept it as one set of exercises so that it's self-contained and doesn't have any prerequisites. For example, we don't want to separate the SAE exercises from the first section (intro to Anthropic's toy models) because we strongly encourage people to at least do the first few exercises in the toy models section & understand the basic ideas behind superposition before trying to tackle SAEs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVN24flw3Tvr"
   },
   "source": [
    "### Toy Models of Superposition (sections 1-3)\n",
    "\n",
    "In these sections, you'll be exposed to Anthropic's toy models of superposition. You'll learn about the key ideas of superposition and why it presents a problem for interpreting neural networks, and you'll experiment with different ways of visualising superposition. You'll also see how properties like sparsity and importance of features affect the degree and nature of superposition.\n",
    "\n",
    "> ##### Learning objectives\n",
    ">\n",
    "> - Understand the concept of superposition, and why models need to do it\n",
    "> - Understand the difference between superposition and polysemanticity\n",
    "> - Understand the difference between neuron and bottleneck superposition (or computational and representational superposition)\n",
    "> - Build & train the toy model from Anthropic's paper, replicate the main results\n",
    "> - See how superposition varies when you change the following characteristics of the features:\n",
    ">   - Importance\n",
    ">   - Sparsity\n",
    ">   - Correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fx_LJFjO3U4B"
   },
   "source": [
    "\n",
    "### Toy Models of Superposition: extensions (sections 4-5)\n",
    "\n",
    "The next two sections take deep dives into some extension topics from the toy models paper: namely the geometry of superposition, and deep double descent. We don't consider these exercises as essential as the previous ones, but they're still very interesting and worth doing if you have time, or just want to dive more deeply into superposition.\n",
    "\n",
    "> ##### Learning objectives\n",
    ">\n",
    "> - Understand the geometric intuitions behind superposition, and how they relate to the more general ideas of superposition in larger models\n",
    "> - Understand how superposition might relate to double descent, and the idea of memorizing vs generalizing solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfu_i7yL3Vdp"
   },
   "source": [
    "\n",
    "### Sparse AutoEncoders (section 6-7)\n",
    "\n",
    "In this last section, you'll learn about sparse autoencoders, and how they might help us resolve problems of superposition. You'll first train an SAE on the toy model setup from earlier sections, then conclude by investigating an SAE trained on a real language model.\n",
    "\n",
    "> ##### Learning objectives (toy models, extensions: sections 6-7)\n",
    ">\n",
    "> - Learn about sparse autoencoders, and how they might be used to disentangle features represented in superposition\n",
    "> - Train your own SAEs on the toy models from earlier sections, and visualise the feature reconstruction process\n",
    "> - Interpret SAEs trained on real language models, and see what kinds of features they can find which are dense in the neuron basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcgAnZZOyBYk"
   },
   "source": [
    "## Setup (don't read, just run!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7yYsYe32yl9U"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install packages\n",
    "    %pip install transformer_lens\n",
    "    %pip install git+https://github.com/callummcdougall/eindex.git\n",
    "\n",
    "    # Code to download the necessary files\n",
    "    import os, sys\n",
    "    from pathlib import Path\n",
    "    if not os.path.exists(\"chapter1_transformers\"):\n",
    "        !curl -o /content/main.zip https://codeload.github.com/callummcdougall/sae-exercises-mats/zip/refs/heads/main\n",
    "        !unzip /content/main.zip\n",
    "        for f in Path(\"sae-exercises-mats-main\").iterdir():\n",
    "            f.rename(f.name)\n",
    "        os.remove(\"/content/main.zip\")\n",
    "        os.rmdir(\"sae-exercises-mats-main\")\n",
    "\n",
    "    # Code to make matplotlib figures work (particularly animations)\n",
    "    from matplotlib import rc\n",
    "    rc('animation', html='jshtml')\n",
    "else:\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "    %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "koQM0J3d5DSy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 15:08:11.803011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-27 15:08:11.865475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-27 15:08:11.866049: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-27 15:08:11.956497: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-27 15:08:13.082320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import torch as t\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from typing import Optional, Callable, Union, List, Tuple\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from rich import print as rprint\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from plotly.express import imshow, line, histogram\n",
    "from plotly_utils_toy_models import (\n",
    "    plot_features_in_2d,\n",
    "    plot_features_in_Nd,\n",
    "    plot_features_in_Nd_discrete,\n",
    "    plot_correlated_features,\n",
    "    plot_feature_geometry,\n",
    "    frac_active_line_plot,\n",
    ")\n",
    "import tests as tests\n",
    "import solutions as solutions\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vUrOoRt5DSy"
   },
   "source": [
    "## Reading Material\n",
    "\n",
    "Here are a few recommended resources to help you get started. Each one is labelled with what you should read, at minimum.\n",
    "\n",
    "Required:\n",
    "\n",
    "* [200 COP in MI: Exploring Polysemanticity and Superposition](https://www.alignmentforum.org/posts/o6ptPu7arZrqRCxyz/200-cop-in-mi-exploring-polysemanticity-and-superposition)\n",
    "    * Read the post, up to and including \"Tips\" (although some parts of it might make more sense after you've read the other things here).\n",
    "* Neel Nanda's [Dynalist notes on superposition](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=3br1psLRIjQCOv2T4RN3V6F2)\n",
    "    * These aren't long, you should skim through them, and also use them as a reference during these exercises.\n",
    "* [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) (Anthropic paper)\n",
    "    * You should read up to & including the \"Summary: A Hierarchy of Feature Properties\" section.\n",
    "    * The first few sections (\"Key Results\", \"Definitions and Motivation\", and \"Empirical Phenomena\" are particularly important).\n",
    "    * We'll also be going through other parts of this paper as we work through the exercises.\n",
    "\n",
    "Optional:\n",
    "\n",
    "* Appendix of [Finding Neurons in a Haystack: Case Studies with Sparse Probing](https://arxiv.org/abs/2305.01610)\n",
    "    * Despite this paper not *just* being about superposition, it has some of the best-written explanations of superposition concepts.\n",
    "    * Sections A.6 - A.9 are particularly good.\n",
    "* Neel Nanda's [video walkthrough of superposition](https://www.youtube.com/watch?v=R3nbXgMnVqQ)\n",
    "    * This is very long and you don't *have* to watch it, but we weakly recommend it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7cvZQUq5DSy"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Here are a set of questions (with some brief answers provided) which you should be able to answer for yourself after reading the above material. Seach for them on Neel's Dynalist notes if you didn't come across them during your reading.\n",
    "\n",
    "What is a **privileged basis**? Why should we expect neuron activations to be privileged by default? Why *shouldn't* we expect the residual stream to be privileged?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "A privileged basis is one where the standard basis vectors are meaningful, i.e. they represent some human-understandable concepts.\n",
    "\n",
    "Neuron activations are privileged because of the **nonlinear function that gets applied**. As an example, consider a simple case of 2 neurons (represented as `x` and `y` coordinates), and suppose we want to store two features in this vector space. If we stored them in non-basis directions, then it would be messy to extract each feature individally (we'd get interference between the two).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/priv-basis.png\" width=\"750\">\n",
    "\n",
    "The residual stream is not privileged because anything that reads from it and writes to it uses a linear map. As a thought experiment, if we changed all the writing matrices (i.e. $W_{out}$ in the MLP layers and $W_O$ in the attention layers) to $W \\to W R$, and all the reading matrices (i.e. $W_{in}$ in the MLP layers and $W_Q$, $W_K$, $W_V$ in the attention layers) to $W \\to W R^{-1}$ where $R$ is some arbitrary rotation matrix, then the model's computation would be unchanged. Since the matrix $R$ is arbitrary, it can change the basis in any way it wants, so that basis can't be privileged.\n",
    "\n",
    "To take this back to the analogy for transformers as [people standing in a line](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers), imagine everyone in the line was speaking and thinking in a different language (which had a 1-1 mapping with English, but which you (an outside observer) didn't have a dictionary for). This wouldn't meaningfully change the way the people in the line were sharing and processing information, it would just change the way the information was stored - and without the dictionary (or any additional context), you can't interpret this information.\n",
    "</details>\n",
    "\n",
    "What is the difference between **neuron superposition** and **neuron polysemanticity**?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "Polysemanticity happens when one neuron corresponds to multiple features (see [here](https://distill.pub/2020/circuits/zoom-in/#:~:text=lot%20of%20effort.-,Polysemantic%20Neurons,-This%20essay%20may) for more discussion & examples). If we only had polysemanticity, this wouldn't really be a problem for us (there might exist a basis for features s.t. each basis vector corresponds to a single feature).\n",
    "\n",
    "Superposition is when there are **more features than neurons**. So it implies polysemanticity (because we must have neurons representing more than one feature), but the converse is not true.\n",
    "</details>\n",
    "\n",
    "\n",
    "What are the **importance** and **sparsity** of features? Do you expect more or less polysemantic neurons if sparsity is larger?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "**Importance** = how useful is this feature for achieving lower loss?\n",
    "\n",
    "**Sparsity** = how frequently is it in the input data?\n",
    "\n",
    "If sparsity is larger, then we expect more polysemantic neurons. This is because a single neuron can afford to represent several different sparse features (usually it'll only be representing one of them at any given time, so there won't be interference).\n",
    "</details>\n",
    "\n",
    "How would you define a **feature**?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "There's no single correct answer to this. Many of the definitions are unfortunately circular (e.g. \"a feature is a thing which could be represented by a neuron\"). A few possible definitions are this one from Neel's [Dynalist notes](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#q=feature):\n",
    "\n",
    "> A feature is a property of an input to the model, or some subset of that input (eg a token in the prompt given to a language model, or a patch of an image).\n",
    "\n",
    "or this similar one from Chris Olah's [Distil Circuits Thread](https://distill.pub/2020/circuits/zoom-in/):\n",
    "\n",
    "> A feature is a a scalar function of the input. In this essay, neural network features are directions, and often simply individual neurons. We claim such features in neural networks are typically meaningful features which can be rigorously studied. A **meaningful feature** is one that genuinely responds to an articulable property of the input, such as the presence of a curve or a floppy ear.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W0zgPVMw0XP"
   },
   "source": [
    "# (1) TMS: Superposition in a Nonprivileged Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utS2IIVc5DSy"
   },
   "source": [
    "## Toy Model - setup\n",
    "\n",
    "In this section, we'll be examining & running experiments on the toy model studied in [Anthropic's paper](https://transformer-circuits.pub/2022/toy_model/index.html).\n",
    "\n",
    "You can follow along with the paper from the [Demonstrating Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating) section onwards; it will approximately follow the order of the sections in this notebook.\n",
    "\n",
    "This paper presented a very rudimentary model for **bottleneck superposition** - when you try and represent more than $n$ features in a vector space of dimension $n$. The model is as follows:\n",
    "\n",
    "* We take a 5-dimensional input $x$\n",
    "* We map it down into 2D space\n",
    "* We map it back up into 5D space (using the transpose of the first matrix)\n",
    "* We add a bias and ReLU\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h &= W x \\\\\n",
    "x' &= \\operatorname{ReLU}(W^T h + b)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jlf27-l05DSy"
   },
   "source": [
    "### What's the motivation for this setup?\n",
    "\n",
    "The input $x$ represents our five features (they're uniformly sampled between 0 and 1).\n",
    "\n",
    "Each feature can have **importance** and **sparsity**. Recall our earlier definitions:\n",
    "\n",
    "* **Importance** = how useful is this feature for achieving lower loss?\n",
    "* **Sparsity** = how frequently is it in the input data?\n",
    "\n",
    "This is realised in our toy model as follows:\n",
    "\n",
    "* **Importance** = the coefficient on the weighted mean squared error between the input and output, which we use for training the model\n",
    "    * In other words, our loss function is $L = \\sum_x \\sum_i I_i (x_i - x_i^\\prime)^2$, where $I_i$ is the importance of feature $i$.\n",
    "* **Sparsity** = the probability of the corresponding element in $x$ being non-zero\n",
    "    * In other words, this affects the way our training data is generated (see the method `generate_batch` in the `Module` class below)\n",
    "\n",
    "The justification for using $W^T W$ is as follows: we can think of $W$ (which is a matrix of shape `(2, 5)`) as a grid of \"overlap values\" between the features and bottleneck dimensions. The values of the 5x5 matrix $W^T W$ are the dot products between the 2D representations of each pair of features. To make this intuition clearer, imagine each of the columns of $W$ were unit vectors, then $W^T W$ would be a matrix of cosine similarities between the features (with diagonal elements equal to 1, because the similarity of a feature with itself is 1). To see this for yourself:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aCnqRWlU5DSy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": {
          "_inputArray": [
           {
            "0": 1.0000001192092896,
            "1": -0.5059776306152344,
            "2": 0.13171416521072388,
            "3": -0.9467092752456665,
            "4": 0.15967920422554016
           },
           {
            "0": -0.5059776306152344,
            "1": 1,
            "2": 0.7883874177932739,
            "3": 0.2011963427066803,
            "4": -0.9322733879089355
           },
           {
            "0": 0.13171416521072388,
            "1": 0.7883874177932739,
            "2": 0.9999998211860657,
            "3": -0.443978488445282,
            "4": -0.9575363993644714
           },
           {
            "0": -0.9467092752456665,
            "1": 0.2011963427066803,
            "2": -0.443978488445282,
            "3": 1.0000001192092896,
            "4": 0.16678708791732788
           },
           {
            "0": 0.15967920422554016,
            "1": -0.9322733879089355,
            "2": -0.9575363993644714,
            "3": 0.16678708791732788,
            "4": 1
           }
          ],
          "bdata": "AQCAP8CHAb8U4AY+iltyv/KCIz7AhwG/AACAP8LTST9qBk4+eKluvxTgBj7C00k//f9/PyZR474bIXW/iltyv2oGTj4mUeO+AQCAPzzKKj7ygiM+eKluvxshdb88yio+AACAPw==",
          "dtype": "f4",
          "shape": "5, 5"
         }
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "height": 360,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cosine similarities of each pair of 2D feature embeddings"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "autorange": true,
         "constrain": "domain",
         "domain": [
          0.28773584905660377,
          0.7122641509433962
         ],
         "range": [
          -0.5,
          4.5
         ],
         "scaleanchor": "y"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "range": [
          4.5,
          -0.5
         ]
        }
       }
      },
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = t.randn(2, 5)\n",
    "W_normed = W / W.norm(dim=0, keepdim=True)\n",
    "\n",
    "imshow(W_normed.T @ W_normed, title=\"Cosine similarities of each pair of 2D feature embeddings\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJv1I4UR5DSz"
   },
   "source": [
    "To put it another way - if the columns of $W$ were orthogonal, then $W^T W$ would be the identity. This can't actually be the case because $W$ is a 2x5 matrix, but its columns can be \"nearly orthgonal\" in the sense of having pairwise cosine similarities close to 0.\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "\n",
    "Question - can you prove that $W^T W$ can't be the identity when $W$ is not a square matrix?\n",
    "</summary>\n",
    "\n",
    "Proof #1: the rank of a matrix product $AB$ is upper-bounded by the maximum of the two factors $A$ and $B$. In the case of $W^T W$, both matrices have rank at most 2, so the product has rank at most 2.\n",
    "\n",
    "Proof #2: for any vector $x$, $W^T W x = W^T (Wx)$ is in the span of the columns of $W^T$, which is vector space with rank 2.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1ksgl3I5DSz"
   },
   "source": [
    "Another nice thing about using two bottleneck dimensions is that we get to visualise our output! We've got a few helper functions for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_normed.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1I9jFL3kvw-i"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "x must be a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_features_in_2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mW_normed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# shape [instances=1 d_hidden=2 features=5]\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/Informatique/4ième année/3ième session/IFT3150 - Projet de recherche d'informatique/sae-exercises-mats/plotly_utils_toy_models.py:541\u001b[0m, in \u001b[0;36mplot_features_in_2d\u001b[0;34m(W, colors, title, subplot_titles, save, colab, n_rows, adjustable_limits)\u001b[0m\n\u001b[1;32m    539\u001b[0m     play(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(save, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/storage/Informatique/4ième année/3ième session/IFT3150 - Projet de recherche d'informatique/sae-exercises-mats/plotly_utils_toy_models.py:513\u001b[0m, in \u001b[0;36mplot_features_in_2d.<locals>.update\u001b[0;34m(val)\u001b[0m\n\u001b[1;32m    511\u001b[0m x, y \u001b[38;5;241m=\u001b[39m W[t][instance_idx][feature_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    512\u001b[0m lines[instance_idx][feature_idx]\u001b[38;5;241m.\u001b[39mset_data([\u001b[38;5;241m0\u001b[39m, x], [\u001b[38;5;241m0\u001b[39m, y])\n\u001b[0;32m--> 513\u001b[0m \u001b[43mmarkers\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstance_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m lines[instance_idx][feature_idx]\u001b[38;5;241m.\u001b[39mset_color(colors[t][instance_idx][feature_idx])\n\u001b[1;32m    515\u001b[0m markers[instance_idx][feature_idx]\u001b[38;5;241m.\u001b[39mset_color(colors[t][instance_idx][feature_idx])\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/matplotlib/lines.py:665\u001b[0m, in \u001b[0;36mLine2D.set_data\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_xdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_ydata(y)\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/matplotlib/lines.py:1289\u001b[0m, in \u001b[0;36mLine2D.set_xdata\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;124;03mSet the data array for x.\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;124;03mset_ydata\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39miterable(x):\n\u001b[0;32m-> 1289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx must be a sequence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xorig \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(x)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: x must be a sequence"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_features_in_2d(\n",
    "    W_normed.unsqueeze(0), # shape [instances=1 d_hidden=2 features=5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoV1OW2D5DSz"
   },
   "source": [
    "Compare this plot to the `imshow` plot above, and make sure you understand what's going on here (and how the two plots relate to each other). A lot of the subsequent exercises run with this idea of a geometric interpretation of the model's features and bottleneck dimensions.\n",
    "\n",
    "<details>\n",
    "<summary>Help - I'm confused about how these plots work.</summary>\n",
    "\n",
    "As mentioned, you can view $W$ as being a set of five 2D vectors, one for each of our five features. The heatmap shows us the cosine similarities between each pair of these vectors, and the second plot shows us these five vectors in 2D space.\n",
    "\n",
    "For example, run the following code@\n",
    "\n",
    "```python\n",
    "t.manual_seed(2)\n",
    "\n",
    "W = t.randn(2, 5)\n",
    "W_normed = W / W.norm(dim=0, keepdim=True)\n",
    "\n",
    "imshow(W_normed.T @ W_normed, title=\"Cosine similarities of each pair of 2D feature embeddings\", width=600)\n",
    "\n",
    "plot_W(W_normed)\n",
    "```\n",
    "\n",
    "In the heatmap, we can see two pairs of vectors (the 1st & 2nd, and the 0th & 4th) have very high cosine similarity. This is reflected in the 2D plot, where these features are very close to each other (the 0th feature is the darkest color, the 4th feature is the lightest).\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-SyxUz05DSz"
   },
   "source": [
    "### Exercise - define your model\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 15-25 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Below is some code for your model. It should be familiar to you if you've already built simple neural networks earlier in this course.\n",
    "\n",
    "For now, you just need to fill in the `__init__` and `forward` functions. As the exercises go on, you'll fill in some more of these functions, but for now you can ignore the others.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "* The `Config` class has an `n_instances` class. This is so we can optimize multiple models at once in a single training loop (this'll be useful later on).\n",
    "    * You should treat this as basically like a batch dimension for your weights: you have multiple weight matrices stacked together, each of which is trained independently & in parallel.\n",
    "    * All your weight & bias matrices will have an `n_instances` dimension at the start.\n",
    "    * Each instance will be trained on different data (the data you generate will also have a `n_instances` dimension), although you'll use the same optimizer for all instances.\n",
    "* The `feature_probability` and `importance` arguments correspond to sparsity and importance of features.\n",
    "    * The formula is `feature_probability = 1 - sparsity`. We'll often refer to the feature probability rather than the sparsity (since we'll often have $p = 1 - S \\approx 0$).\n",
    "    * The feature probability is used to generate our training data; the importance is used in our loss function (see later for both of these).\n",
    "\n",
    "You should fill in the `__init__` function, which defines `self.W` and `self.b_final` (see the type annotations). Make sure that `W` is initialized with the [Xavier normal method](https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1a86191a828a085e1c720dbce185d6c307.html). `b_final` can be initialized with zeros.\n",
    "\n",
    "You should also fill in the `forward` function, to calculate the output (again, the type annotations should be helpful here).\n",
    "\n",
    "You will fill out the `generate_batch` and `calculate_loss` functions in later exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRQ9j4ftyHXf"
   },
   "outputs": [],
   "source": [
    "def linear_lr(step, steps):\n",
    "    return (1 - (step / steps))\n",
    "\n",
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "    return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # We optimize n_instances models in a single training loop to let us sweep over\n",
    "    # sparsity or importance curves  efficiently. You should treat `n_instances` as\n",
    "    # kinda like a batch dimension, but one which is built into our training setup.\n",
    "    n_instances: int\n",
    "    n_features: int = 5\n",
    "    n_hidden: int = 2\n",
    "    n_correlated_pairs: int = 0\n",
    "    n_anticorrelated_pairs: int = 0\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    "    b_final: Float[Tensor, \"n_instances n_features\"]\n",
    "    # Our linear map is x -> ReLU(W.T @ W @ x + b_final)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: Optional[Union[float, Tensor]] = None,\n",
    "        importance: Optional[Union[float, Tensor]] = None,\n",
    "        device = device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... instances features\"]\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_batch(self, batch_size) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "        '''\n",
    "        Generates a batch of data. We'll return to this function later when we apply correlations.\n",
    "        '''\n",
    "        pass # See below for solutions\n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch instances features\"],\n",
    "        batch: Float[Tensor, \"batch instances features\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        '''\n",
    "        Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n",
    "\n",
    "            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
    "\n",
    "        Note, `model.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n",
    "        '''\n",
    "        pass # See below for solutions\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 100,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "    ):\n",
    "        '''\n",
    "        Optimizes the model using the given hyperparameters.\n",
    "        '''\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "\n",
    "        progress_bar = tqdm(range(steps))\n",
    "\n",
    "        for step in progress_bar:\n",
    "\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = step_lr\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            batch = self.generate_batch(batch_size)\n",
    "            out = self(batch)\n",
    "            loss = self.calculate_loss(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display progress bar\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(loss=loss.item()/self.cfg.n_instances, lr=step_lr)\n",
    "\n",
    "\n",
    "tests.test_model(Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUgXCy1w5DSz"
   },
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "\n",
    "    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    "    b_final: Float[Tensor, \"n_instances n_features\"]\n",
    "    # Our linear map (ignoring n_instances) is x -> ReLU(W.T @ W @ x + b_final)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Config,\n",
    "        feature_probability: Optional[Tensor] = None,\n",
    "        importance: Optional[Tensor] = None,\n",
    "        device=device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        if feature_probability is None: feature_probability = t.ones(())\n",
    "        self.feature_probability = feature_probability.to(device)\n",
    "        if importance is None: importance = t.ones(())\n",
    "        self.importance = importance.to(device)\n",
    "\n",
    "        # SOLUTION\n",
    "        self.W = nn.Parameter(t.empty((config.n_instances, config.n_hidden, config.n_features), device=device))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        self.b_final = nn.Parameter(t.zeros((config.n_instances, config.n_features), device=device))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... instances features\"]\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "        # SOLUTION\n",
    "        hidden = einops.einsum(\n",
    "           features, self.W,\n",
    "           \"... instances features, instances hidden features -> ... instances hidden\"\n",
    "        )\n",
    "        out = einops.einsum(\n",
    "            hidden, self.W,\n",
    "            \"... instances hidden, instances hidden features -> ... instances features\"\n",
    "        )\n",
    "        out = out + self.b_final\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def generate_batch(self, n_batch) -> Float[Tensor, \"n_batch instances features\"]:\n",
    "        '''\n",
    "        Generates a batch of data. We'll return to this function later when we apply correlations.\n",
    "        '''\n",
    "        # SOLUTION\n",
    "        feat = t.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device)\n",
    "        feat_seeds = t.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device)\n",
    "        feat_is_present = feat_seeds <= self.feature_probability\n",
    "        batch = t.where(\n",
    "            feat_is_present,\n",
    "            feat,\n",
    "            t.zeros((), device=self.W.device),\n",
    "        )\n",
    "        return batch\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZElPqnT5DS0"
   },
   "source": [
    "### Exercise - implement the `generate_batch` method\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 10-15 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Next, you should implement the function `generate_batch` above. This should return a tensor of shape `(n_batch, instances, features)`, where:\n",
    "\n",
    "* The `instances` and `features` values are taken from the model config,\n",
    "* Each feature is present with probability `self.feature_probability`,\n",
    "* Each present feature is sampled from a uniform distribution between 0 and 1.\n",
    "\n",
    "Make sure you understand this function well (we recommend looking at the solutions even after you pass the tests), because we'll be making more complicated versions of this function in the section on correlations.\n",
    "\n",
    "Note - `self.feature_probability` is guaranteed to broadcast with the `(n_batch, instances, features)` shape. You can read the code in `Model.__init__` which makes sure this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERULbPlfCkP5"
   },
   "outputs": [],
   "source": [
    "def generate_batch(self: Model, batch_size) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "    '''\n",
    "    Generates a batch of data. We'll return to this function later when we apply correlations.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST8lutvB5DS0"
   },
   "outputs": [],
   "source": [
    "Model.generate_batch = generate_batch\n",
    "\n",
    "tests.test_generate_batch(Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtjKlA3D5DS0"
   },
   "source": [
    "## Training our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmYChnxI5DS0"
   },
   "source": [
    "The details of training aren't very conceptually important, so we've given you most of the code to train the model below. We use **learning rate schedulers** to control the learning rate as the model trains - you'll use this later on during the RL chapter.\n",
    "\n",
    "### Exercise - implement the `calculate_loss` method\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 5-10 minutes on this exercise.\n",
    "```\n",
    "\n",
    "You should fill in the `calculate_loss` function below. The loss function **for a single instance** is given by:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{BF}\\sum_x \\sum_i I_i\\left(x_i-x_i^{\\prime}\\right)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $B$ is the batch size,\n",
    "* $F$ is the number of features,\n",
    "* $x_i$ are the inputs and $x_i'$ are the model's outputs,\n",
    "* $I_i$ is the importance of feature $i$,\n",
    "* $\\sum_i$ is a sum over features,\n",
    "* $\\sum_x$ is a sum over the elements in the batch.\n",
    "\n",
    "For the general case, we sum this formula over all instances.\n",
    "\n",
    "<details>\n",
    "<summary>Question - why do you think we take the mean over the feature and batch dimensions, but we sum over the instances dimension?</summary>\n",
    "\n",
    "We take the mean over batch size because this is standard for loss functions (and means we don't have to use a different learning rate for different batch sizes).\n",
    "\n",
    "We take the mean over the feature dimension because that's [normal for MSE loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).\n",
    "\n",
    "We sum over the instances dimension because we want to train each instance independently, and at the same rate as we would train a single instance.\n",
    "\n",
    "</details>\n",
    "\n",
    "Note - `self.importance` is guaranteed to broadcast with the `(n_batch, instances, features)` shape, just like `self.feature_probability`. You can read the code in `Model.__init__` which makes sure this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67xWrkkfCqF4"
   },
   "outputs": [],
   "source": [
    "def calculate_loss(\n",
    "    self: Model,\n",
    "    out: Float[Tensor, \"batch instances features\"],\n",
    "    batch: Float[Tensor, \"batch instances features\"],\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    '''\n",
    "    Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n",
    "\n",
    "        https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
    "\n",
    "    Note, `self.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIoEvKhy5DS0"
   },
   "outputs": [],
   "source": [
    "Model.calculate_loss = calculate_loss\n",
    "\n",
    "tests.test_calculate_loss(Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXnpzusQ5DS0"
   },
   "source": [
    "Now, we'll reproduce a version of the figure from the introduction, although with a slightly different version of the code.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "* The `importance` argument is the same for all instances. It takes values between 1 and ~0.66 for each feature (so for every instance, there will be some features which are more important than others).\n",
    "* The `feature_probability` is the same for all features, but it varies across instances. In other words, we're runnning several different experiments at once, and we can compare the effect of having larger feature sparsity in these experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36IE7Q2i5DS0"
   },
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    n_instances = 8,\n",
    "    n_features = 5,\n",
    "    n_hidden = 2,\n",
    ")\n",
    "\n",
    "# importance varies within features for each instance\n",
    "importance = (0.9 ** t.arange(cfg.n_features))\n",
    "importance = einops.rearrange(importance, \"features -> () features\")\n",
    "\n",
    "# sparsity is the same for all features in a given instance, but varies over instances\n",
    "feature_probability = (50 ** -t.linspace(0, 1, cfg.n_instances))\n",
    "feature_probability = einops.rearrange(feature_probability, \"instances -> instances ()\")\n",
    "\n",
    "line(importance.squeeze(), width=600, height=400, title=\"Importance of each feature (same over all instances)\", labels={\"y\": \"Feature importance\", \"x\": \"Feature\"})\n",
    "line(feature_probability.squeeze(), width=600, height=400, title=\"Feature probability (varied over instances)\", labels={\"y\": \"Probability\", \"x\": \"Instance\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CN_lZC_95DS1"
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    importance = importance,\n",
    "    feature_probability = feature_probability,\n",
    ")\n",
    "model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIfWALMz2cKG"
   },
   "outputs": [],
   "source": [
    "plot_features_in_2d(\n",
    "    model.W.detach(),\n",
    "    colors = model.importance,\n",
    "    title = \"Superposition: 5 features represented in 2D space\",\n",
    "    subplot_titles = [f\"1 - S = {i:.3f}\" for i in feature_probability.squeeze()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okuSmfVTvw-l"
   },
   "source": [
    "<details>\n",
    "<summary>Click this dropdown to see what you should be getting from this visualisation.</summary>\n",
    "\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/sp1.png\" width=\"1400\">\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoeqf0X_5DS1"
   },
   "source": [
    "### Exercise - interpret these diagrams\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴⚪⚪\n",
    "Importance: 🔵🔵🔵🔵⚪\n",
    "\n",
    "You should spend up to 10-20 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Remember that for all these diagrams, the darker colors have lower importance and the lighter colors have higher importance. Also, the sparsity of all features is increasing as we move from left to right (at the far left there is no sparsity, at the far right feature probability is 5% for all features, i.e. sparsity of 95%).\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "For low sparsity, think about what the model would learn to do if all 5 features were present all the time. What's the best our model could do in this case, and how does that relate to the **importance** values?\n",
    "\n",
    "For high sparsity, think about what the model would learn to do if there was always exactly one feature present. Does this make interference between features less of a problem?\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer (intuitive)</summary>\n",
    "\n",
    "When there is no sparsity, the model can never represent more than 2 features faithfully, so it makes sense for it to only represent the two most important features. It stores them orthogonally in 2D space, and sets the other 3 features to zero. This way, it can reconstruct these two features perfectly, and ignores all the rest.\n",
    "\n",
    "When there is high sparsity, we get a pentagon structure. Most of the time at most one of these five features will be active, which helps avoid **interference** between features. When we try to recover our initial features by projecting our point in 2D space onto these five directions, most of the time when feature $i$ is present, we can be confident that our projection onto the $i$-th feature direction only captures this feature, rather than being affected by the presence of other features. We omit the mathematical details here.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/download (7).png\" width=\"900\">\n",
    "\n",
    "The key idea here is that two forces are competing in our model: **feature benefit** (representing more thing is good!), and **interference** (representing things non-orthogonally is bad). The higher the sparsity, the more we can reduce the negative impact of interference, and so the trade-off skews towards \"represent more features, non-orthogonally\".\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHrZbz2Ovw-l"
   },
   "source": [
    "We can also generate a batch and visualise its embedding. Most interestingly, you should see that in the plots with high sparsity (to the right), we very rarely have interference between the five features, because most often $\\leq 1$ of those features is present, and the model can recover it by projecting along the corresponding feature dimension without losing any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDe0QWPRvw-l"
   },
   "outputs": [],
   "source": [
    "with t.inference_mode():\n",
    "    batch = model.generate_batch(200)\n",
    "    hidden = einops.einsum(batch, model.W, \"batch_size instances features, instances hidden features -> instances hidden batch_size\")\n",
    "\n",
    "plot_features_in_2d(hidden, title = \"Hidden state representation of a random batch of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUzv08Tpvw-l"
   },
   "source": [
    "<details>\n",
    "<summary>Click this dropdown to see what you should be getting from this visualisation.</summary>\n",
    "\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/hs1.png\" width=\"1400\">\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1RzkWqn5DS1"
   },
   "source": [
    "## Visualizing features across varying sparsity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPHR2UBZ5DS1"
   },
   "source": [
    "Now that we've got our pentagon plots and started to get geometric intuition for what's going on, let's scale things up! We're now operating in dimensions too large to visualise, but hopefully our intuitions will carry over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWGU8wAn5DS1"
   },
   "outputs": [],
   "source": [
    "n_features = 80\n",
    "n_hidden = 20\n",
    "\n",
    "importance = (0.9 ** t.arange(n_features))\n",
    "importance = einops.rearrange(importance, \"features -> () features\")\n",
    "\n",
    "# feature_probability = (20 ** -t.linspace(0, 1, cfg.n_instances))\n",
    "feature_probability = t.tensor([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001])\n",
    "feature_probability = einops.rearrange(feature_probability, \"instances -> instances ()\")\n",
    "\n",
    "cfg = Config(\n",
    "    n_instances = len(feature_probability.squeeze()),\n",
    "    n_features = n_features,\n",
    "    n_hidden = n_hidden,\n",
    ")\n",
    "\n",
    "line(importance.squeeze(), width=600, height=400, title=\"Importance of each feature (same over all instances)\", labels={\"y\": \"Feature importance\", \"x\": \"Feature\"})\n",
    "line(feature_probability.squeeze(), width=600, height=400, title=\"Feature probability (varied over instances)\", labels={\"y\": \"Probability\", \"x\": \"Instance\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1O9Jb7nvw-o"
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    importance = importance,\n",
    "    feature_probability = feature_probability,\n",
    ")\n",
    "model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9uXMREt5DS1"
   },
   "source": [
    "Because we can't plot features in 2D anymore, we're going to use a different kind of visualisation:\n",
    "\n",
    "* The **left hand plots** shows a bar graph of all the features and their corresponding embedding norms $||W_i||$.\n",
    "    * As we increase sparsity, the model is able to represent more features (i.e. we have more features with embedding norms close to 1).\n",
    "    * We also color the bars according to whether they're orthogonal to other features (blue) or not (red). So we can see that for low sparsity most features are represented orthogonally (like our left-most plots above) but as we increase sparsity we transition to all features being represented non-orthogonally (like our right-most pentagon plots above).\n",
    "* The **right hand plots** show us the dot products between all pairs of feature vectors (kinda like the heatmaps we plotted at the start of this section).\n",
    "    * This is another way of visualising the increasing interference between features as we increase sparsity.\n",
    "    * Note that all these right hand plots represent **matrices with rank at most `n_hidden=20`**. The first few are approximately submatrices of the identity (because we perfectly reconstruct 20 features and delete the rest), but the later plots start to display inference as we plot more than 20 values (the diagonals of these matrices have more than 20 non-zero elements).\n",
    "\n",
    "See the section [Basic Results](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-basic-results) for more of an explanation of this graph and what you should interpret from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVGrWm790Z_z"
   },
   "outputs": [],
   "source": [
    "plot_features_in_Nd(\n",
    "    model.W,\n",
    "    height = 600,\n",
    "    width = 1400,\n",
    "    title = \"ReLU output model: n_features = 80, d_hidden = 20, I<sub>i</sub> = 0.9<sup>i</sup>\",\n",
    "    subplot_titles = [f\"Feature prob = {i:.3f}\" for i in feature_probability[:, 0]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq_Vqe_u2cKL"
   },
   "source": [
    "## Bonus - varying feature probability across features\n",
    "\n",
    "In this section, we've only discussed varying feature probability across instances, and so most of the learned solutions have had uniformity (e.g. a uniform pentagon, or a uniform digon with one feature collapsed). But there's also a large set of non-uniform patterns which can be learned by our models. See the [corresponding section](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-non-uniform) of the Anthropic paper, where they discuss in more detail what happens when a feature's importance is perturbed. Can you reproduce this result? Can you think of a setup which would result in a learned solution where all 5 features are represented, but two features are represented with very high cosine similarity? (You might want to return to this question at the end of the next section!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cBVpwSy2cKL"
   },
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    n_instances = 16,\n",
    "    n_features = 5,\n",
    "    n_hidden = 2,\n",
    ")\n",
    "\n",
    "# now we vary feature probability within features (but same for all instances)\n",
    "# we make all probs 0.05, except for the first feature which has smaller probability\n",
    "feature_probability = t.full((cfg.n_instances, cfg.n_features), 0.05)\n",
    "feature_probability[:, 0] *= t.linspace(0, 1, cfg.n_instances+1)[1:].flip(0)\n",
    "\n",
    "model = Model(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    feature_probability = feature_probability,\n",
    ")\n",
    "model.optimize(steps=25_000)\n",
    "\n",
    "plot_features_in_2d(\n",
    "    model.W,\n",
    "    colors = model.feature_probability * (1 / model.feature_probability.max()), # to help distinguish colors, we normalize to use the full color range\n",
    "    title = \"Superposition: 5 features represented in 2D space (lighter colors = larger feature probabilities)\",\n",
    "    subplot_titles = [f\"1 - S = 0.05 * {i:.2f}\" for i in t.linspace(0, 1, cfg.n_instances).flip(0)],\n",
    "    n_rows = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMUtONps5DS4"
   },
   "source": [
    "# (2) TMS: Correlated / Anticorrelated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efN1F9no5DS5"
   },
   "source": [
    "One major thing we haven't considered in our experiments is **correlation**. We could guess that superposition is even more common when features are **anticorrelated** (for a similar reason as why it's more common when features are sparse). Most real-world features are anticorrelated (e.g. the feature \"this is a sorted Python list\" and \"this is some text in an edgy teen vampire romance novel\" are probably anticorrelated - that is, unless you've been reading some pretty weird fanfics).\n",
    "\n",
    "In this section, you'll define a new data-generating function for correlated features, and run the same experiments as in the first section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWXvuxOS5DS5"
   },
   "source": [
    "### Exercise - implement `generate_correlated_batch`\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴🔴⚪\n",
    "Importance: 🔵🔵⚪⚪⚪\n",
    "\n",
    "You should spend up to 20-40 minutes on this exercise.\n",
    "\n",
    "The exercise itself is not conceptually important, and it is a bit fiddly / delicate, so you should definitely look at the solutions if you get stuck. Understanding the results and why they occur is more important than the implementation!\n",
    "```\n",
    "\n",
    "You should now fill in the three methods `generate_correlated_features`, `generate_anticorrelated_features` and `generate_uncorrelated_features` in the `Model` class, which are created to generate correlated / anticorrelated data. A summary of what you will have to do:\n",
    "\n",
    "* The `generate_correlated_features` function returns a tensor of shape `(batch_size, n_instances, n_features)`, where `n_features = 2 * n_correlated_pairs`, and each feature is correlated with the next feature.\n",
    "    * In other words, each `output[i, j, 2k]` and `output[i, j, 2k+1]` are correlated: one is non-zero iff the other is non-zero.\n",
    "    * Hint - create a tensor of random seeds of shape `(batch_size, n_instances, n_correlated_pairs)`, then use `einops.repeat`.\n",
    "* The `generate_anticorrelated_features` function returns a tensor of shape `(batch_size, n_instances, n_features)`, where `n_features = 2 * n_anticorrelated_pairs`, and each feature is anticorrelated with the next feature.\n",
    "    * In other words, each `output[i, j, 2k]` and `output[i, j, 2k+1]` are anticorrelated: one is non-zero iff the other is zero.\n",
    "    * Hint - create a tensor of random seeds of shape `(batch_size, n_instances, n_anticorrelated_pairs)` which determines whether one of the features in a given pair is zero or not, and then for the non-zero features you can use another random seed tensor to determine whether the first or the second feature is non-zero.\n",
    "    * Note - if `p` is the feature probability, then you should have the probability of any given pair of anticorrelated features being present as `2p` - this is so that any single feature has probability `2p * (1/2) = p` of being present.\n",
    "* The `generate_uncorrelated_features` function returns a tensor of shape `(batch_size, n_instances, n_uncorrelated)`.\n",
    "    * This should be exactly the same function as your current version of `generate_batch`.\n",
    "\n",
    "For these functions you can assume that the `model.feature_probability` is the same for all features (although it might vary across instances). Since `model.feature_probability` automatically gets broadcasted to shape `(n_instances, n_features)` during intialization, you can handle this by just indexing the probability for the first feature with e.g. `[:, 0]` to get a vector of length `n_instances`.\n",
    "\n",
    "For more details, you can read the [experimental details in Anthropic's paper](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-correlated-setup), where they describe how they setup correlated and anticorrelated sets.\n",
    "\n",
    "You should fill in the functions below. Note that we've also given you a new `generate_batch` function, which calculates how many correlated / anticorrelated / uncorrelated features we need, and creates a full batch by concatenating the outputs of these three functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1Gx1k8gCbJZ"
   },
   "outputs": [],
   "source": [
    "def generate_correlated_features(self: Model, batch_size, n_correlated_pairs) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "    '''\n",
    "    Generates a batch of correlated features.\n",
    "    Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_anticorrelated_features(self: Model, batch_size, n_anticorrelated_pairs) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "    '''\n",
    "    Generates a batch of anti-correlated features.\n",
    "    Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_uncorrelated_features(self: Model, batch_size, n_uncorrelated) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "    '''\n",
    "    Generates a batch of uncorrelated features.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_batch(self: Model, batch_size):\n",
    "    '''\n",
    "    Generates a batch of data, with optional correslated & anticorrelated features.\n",
    "    '''\n",
    "    n_uncorrelated = self.cfg.n_features - 2 * self.cfg.n_correlated_pairs - 2 * self.cfg.n_anticorrelated_pairs\n",
    "    data = []\n",
    "    if self.cfg.n_correlated_pairs > 0:\n",
    "        data.append(self.generate_correlated_features(batch_size, self.cfg.n_correlated_pairs))\n",
    "    if self.cfg.n_anticorrelated_pairs > 0:\n",
    "        data.append(self.generate_anticorrelated_features(batch_size, self.cfg.n_anticorrelated_pairs))\n",
    "    if n_uncorrelated > 0:\n",
    "        data.append(self.generate_uncorrelated_features(batch_size, n_uncorrelated))\n",
    "    batch = t.cat(data, dim=-1)\n",
    "    return batch\n",
    "\n",
    "\n",
    "Model.generate_correlated_features = generate_correlated_features\n",
    "Model.generate_anticorrelated_features = generate_anticorrelated_features\n",
    "Model.generate_uncorrelated_features = generate_uncorrelated_features\n",
    "Model.generate_batch = generate_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e22Bzep5DS5"
   },
   "source": [
    "The code below tests your function, by generating a large number of batches and measuring them statistically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAzw5QzW5DS5"
   },
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    n_instances = 30,\n",
    "    n_features = 4,\n",
    "    n_hidden = 2,\n",
    "    n_correlated_pairs = 1,\n",
    "    n_anticorrelated_pairs = 1,\n",
    ")\n",
    "\n",
    "feature_probability = 10 ** -t.linspace(0.5, 1, cfg.n_instances).to(device)\n",
    "\n",
    "model = Model(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    feature_probability = einops.rearrange(feature_probability, \"instances -> instances ()\")\n",
    ")\n",
    "\n",
    "# Generate a batch of 4 features: first 2 are correlated, second 2 are anticorrelated\n",
    "batch = model.generate_batch(batch_size=100_000)\n",
    "corr0, corr1, anticorr0, anticorr1 = batch.unbind(dim=-1)\n",
    "corr0_is_active = corr0 != 0\n",
    "corr1_is_active = corr1 != 0\n",
    "anticorr0_is_active = anticorr0 != 0\n",
    "anticorr1_is_active = anticorr1 != 0\n",
    "\n",
    "assert (corr0_is_active == corr1_is_active).all(), \"Correlated features should be active together\"\n",
    "assert (corr0_is_active.float().mean(0) - feature_probability).abs().mean() < 0.002, \"Each correlated feature should be active with probability `feature_probability`\"\n",
    "\n",
    "assert (anticorr0_is_active & anticorr1_is_active).int().sum().item() == 0, \"Anticorrelated features should never be active together\"\n",
    "assert (anticorr0_is_active.float().mean(0) - feature_probability).abs().mean() < 0.002, \"Each anticorrelated feature should be active with probability `feature_probability`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPbBOuMs2cKL"
   },
   "source": [
    "We can also visualise these features, in the form of a bar chart. You should see the correlated features always co-occurring, and the anticorrelated features never co-occurring.\n",
    "\n",
    "<details>\n",
    "<summary>What you should see when you run the code below</summary>\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/bar-cooccur.png\" width=\"800\">\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-ht50Dy5DS5"
   },
   "outputs": [],
   "source": [
    "# Generate a batch of 4 features: first 2 are correlated, second 2 are anticorrelated\n",
    "batch = model.generate_batch(batch_size = 1)\n",
    "correlated_feature_batch, anticorrelated_feature_batch = batch[:, :, :2], batch[:, :, 2:]\n",
    "\n",
    "# Plot correlated features\n",
    "plot_correlated_features(correlated_feature_batch, title=\"Correlated Features: should always co-occur\")\n",
    "plot_correlated_features(anticorrelated_feature_batch, title=\"Anti-correlated Features: should never co-occur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoHAQLUb5DS6"
   },
   "source": [
    "Now, let's try training our model & visualising features in 2D, when we have 2 pairs of correlated features (matching the [first figure](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-organization) in the Anthropic paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzdACaa2vw-p"
   },
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    n_instances = 5,\n",
    "    n_features = 4,\n",
    "    n_hidden = 2,\n",
    "    n_correlated_pairs = 2,\n",
    "    n_anticorrelated_pairs = 0,\n",
    ")\n",
    "\n",
    "# All same importance, very low feature probabilities (ranging from 5% down to 0.25%)\n",
    "importance = t.ones(cfg.n_features, dtype=t.float, device=device)\n",
    "importance = einops.rearrange(importance, \"features -> () features\")\n",
    "feature_probability = (400 ** -t.linspace(0.5, 1, cfg.n_instances))\n",
    "feature_probability = einops.rearrange(feature_probability, \"instances -> instances ()\")\n",
    "\n",
    "model = Model(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    importance = importance,\n",
    "    feature_probability = feature_probability,\n",
    ")\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vu6XaI_J2cKL"
   },
   "outputs": [],
   "source": [
    "plot_features_in_2d(\n",
    "    model.W,\n",
    "    colors = [\"blue\"] * 2 + [\"limegreen\"] * 2, # when colors is a list of strings, it's assumed to be the colors of features\n",
    "    title = \"Correlated feature sets are represented in local orthogonal bases\",\n",
    "    subplot_titles = [f\"1 - S = {i:.3f}\" for i in model.feature_probability[:, 0]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FWQPBs95DS6"
   },
   "source": [
    "### Exercise - generate more feature correlation plots\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to ~10 minutes on this exercise.\n",
    "\n",
    "It should just involve changing the parameters in your code above.\n",
    "```\n",
    "\n",
    "You should now plot the second and third figures from the paper. You may not get exactly the same results as the paper, but they should still roughly match (e.g. you should see no antipodal pairs in the code above, but you should see at least some when you test the anticorrelated sets, even if not all of them are antipodal). You can look at the solutions colab to see some examples.\n",
    "\n",
    "<details>\n",
    "<summary>Question - for the anticorrelated feature plots, you'l have to increase the feature probability to something like ~10%, or else you won't always form antipodal pairs. Why do you think this is?</summary>\n",
    "\n",
    "If sparsity is small / feature prob is large, then interference between the two pairs of anticorrelated features is a problem. If two features from different pairs are in the same subspace (because they're antipodal) the model is more likely to keep looking for a better solution.\n",
    "\n",
    "On the other hand, if sparsity is very large / feature probability is close to zero, then the negative effect of interference is much smaller. So the difference in loss between the solutions where the antipodal pairs are / aren't the same as the anticorrelated pairs is much smaller, and the model is more likely to just settle on whichever solution it finds first.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTzBhMGj5DS6"
   },
   "source": [
    "# (3) TMS: Superposition in a Privileged Basis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4vSP5-45DS6"
   },
   "source": [
    "So far, we've explored superposition in a model without a privileged basis. We can rotate the hidden activations arbitrarily and, as long as we rotate all the weights, have the exact same model behavior. That is, for any ReLU output model with weights\n",
    "$W$, we could take an arbitrary orthogonal matrix $O$ and consider the model $W' = OW$. Since $(OW)^T(OW) = W^T W$, the result would be an identical model!\n",
    "\n",
    "Models without a privileged basis are elegant, and can be an interesting analogue for certain neural network representations which don't have a privileged basis – word embeddings, or the transformer residual stream. But we'd also (and perhaps primarily) like to understand neural network representations where there are neurons which do impose a privileged basis, such as transformer MLP layers or conv net neurons.\n",
    "\n",
    "Our goal in this section is to explore the simplest toy model which gives us a privileged basis. There are at least two ways we could do this: we could add an activation function or apply $L_1$ regularization to the hidden layer. We'll focus on adding an activation function, since the representation we are most interested in understanding is hidden layers with neurons, such as the transformer MLP layer.\n",
    "\n",
    "This gives us the following \"ReLU hidden layer\" model. It's the simplest one we can use which is still likely to give us a privileged basis; we just take our previous setup and apply ReLU to the hidden layer.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h & =\\operatorname{ReLU}(W x) \\\\\n",
    "x^{\\prime} & =\\operatorname{ReLU}\\left(W^T h+b\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Mek_9Pk5DS7"
   },
   "source": [
    "### Exercise - implement `NeuronModel`\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to ~10 minutes on this exercise.\n",
    "```\n",
    "\n",
    "In this section, you'll replicate the [first set of results](https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss:~:text=model%20and%20a-,ReLU%20hidden%20layer%20model,-%3A) in the Anthropic paper on studying superposition in a privileged basis. To do this, you'll need a new `NeuronModel` class. It can inherit most methods from the `Model` class, but you'll need to redefine the `forward` method to include an intermediate ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5MCJA2I5DS7"
   },
   "outputs": [],
   "source": [
    "class NeuronModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: Optional[Tensor] = None,\n",
    "        importance: Optional[Tensor] = None,\n",
    "        device=device\n",
    "    ):\n",
    "        super().__init__(cfg, feature_probability, importance, device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... instances features\"]\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "        pass\n",
    "\n",
    "\n",
    "tests.test_neuron_model(NeuronModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qqhtNeH2cKM"
   },
   "source": [
    "Once you've passed these tests, you can run the cells below to train the model in the same way as before. We use just one instance, with zero sparsity and uniform importance.\n",
    "\n",
    "We also visualize the matrix $W$. With the argument `neuron_plot=True`, we make it so that the right-hand visualisation is of $W$ rather than $W^T W$ - we can get away with this now because (unlike before) the individual elements of $W$ *are* meaningful. We're working with a **privileged basis**, and $W$ connects features to basis-aligned neurons.\n",
    "\n",
    "You might find small deviations from the paper's results. But the most important thing to pay attention to is how **there's a shift from monosemantic to polysemantic neurons as sparsity increases**. Monosemantic neurons do exist in some regimes! Polysemantic neurons exist in others. And they can both exist in the same model! Moreover, while it's not quite clear how to formalize this, it looks a great deal like there's a neuron-level phase change, mirroring the feature phase changes we saw earlier.\n",
    "\n",
    "In the plots you make below, you should see:\n",
    "\n",
    "* Total monosemanticity at 5 features & 5 neurons\n",
    "* With more features than neurons, some of the neurons become polysemantic (but some remain monosemantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijdzLwDBzj0i"
   },
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "n_hidden = 5\n",
    "\n",
    "importance = einops.rearrange(0.75 ** t.arange(1, 1+n_features), \"feats -> () feats\")\n",
    "feature_probability = einops.rearrange(t.tensor([0.75, 0.35, 0.15, 0.1, 0.06, 0.02, 0.01]), \"instances -> instances ()\")\n",
    "\n",
    "cfg = Config(\n",
    "    n_instances = len(feature_probability.squeeze()),\n",
    "    n_features = n_features,\n",
    "    n_hidden = n_hidden,\n",
    ")\n",
    "\n",
    "model = NeuronModel(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    importance = importance,\n",
    "    feature_probability = feature_probability,\n",
    ")\n",
    "model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo42Zn3D2cKM"
   },
   "outputs": [],
   "source": [
    "plot_features_in_Nd(\n",
    "    model.W,\n",
    "    height = 600,\n",
    "    width = 1000,\n",
    "    title = \"Neuron model: n_features = 10, d_hidden = 5, I<sub>i</sub> = 0.75<sup>i</sup>\",\n",
    "    subplot_titles = [f\"1 - S = {i:.2f}\" for i in feature_probability.squeeze()],\n",
    "    neuron_plot = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf3J5psb5DS7"
   },
   "source": [
    "Try playing around with different settings (sparsity, importance). What kind of results do you get?\n",
    "\n",
    "You can also try and go further, replicating results later in the paper (e.g. the neuron weight bar plots further on in the paper).\n",
    "\n",
    "(Note - the argument `show_wtw = False` in the `visualise_Nd_superposition` function means that the right-hand heatmap we see isn't $W^T W$, but just $W$. If you set `show_wtw = False` in the previous section you'd have seen something with no visible pattern because the features had no reason to be monosemantic, but here you should be able to see some monosemantic neurons.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx7HRrQA2cKN"
   },
   "source": [
    "## Computation in superposition\n",
    "\n",
    "The example above was interesting, but in some ways it was also limited. The key problem here is that **the model doesn't benefit from the ReLU hidden layer**. Adding a ReLU does encourage the model to have a privileged basis, but since the model is trying to reconstruct the input (i.e. the identity, which is a linear function) it doesn't actually need to use the ReLU, and it will try anything it can to circumvent it - including learning biases which shift all the neurons into a positive regime where they behave linearly. This is a mark against using this toy model to study superposition.\n",
    "\n",
    "To extend this point: we don't want to study boring linear functions like the identity, we want to study **how models perform (nonlinear) computation in superposition**. The MLP layer in a transformer isn't just a way to represent information faithfully and recover it; it's a way to perform computation on that information. So for this next section, we'll train a model to perform some non-linear computation. Specifically, we'll train our model to **compute the absolute value of inputs $x$**.\n",
    "\n",
    "Our data $x$ are now sampled from the range $[-1, 1]$ rather than $[0, 1]$ (otherwise calculating the absolute value would be equivalent to reconstructing the input). This is about as simple as a nonlinear function can get, since $abs(x)$ is equivalent to $\\operatorname{ReLU}(x) + \\operatorname{ReLU}(-x)$. But since it's nonlinear, we can be sure that the model has to use the hidden layer ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHcWSstg2cKN"
   },
   "source": [
    "```c\n",
    "Difficulty: 🔴🔴🔴⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 15-25 minutes on this exercise.\n",
    "```\n",
    "\n",
    "You should fill in the `NeuronComputationModel` class below. Specifically, you'll need to fill in the `forward`, `generate_batch` and `calculate_loss` methods. Some guidance:\n",
    "\n",
    "* The model has a ReLU hidden layer in its forward function (as described above & in the paper). This will require rewriting the `forward` method, but you can keep the same `__init__` method as for the `Model` class (since the weights are the same).\n",
    "    * We've given you the `__init__` method already. It runs the `__init__` method of the `Model` class, but then deletes the `W` matrix and replaces it with `W1` and `W2`.\n",
    "* The model's data is different - see the discussion above. Your `generate_batch` function should be rewritten - it will be the same as the first version of this function you wrote (i.e. without correlations) except for one difference: the value is sampled uniformly from the range $[-1, 1]$ rather than $[0, 1]$.\n",
    "* The model's loss function is different. Rather than computing the (importance-weighted) $L_2$ error between the input $x$ and output $x'$, we're computing the $L_2$ error between $\\operatorname{abs}(x)$ and $x'$. This should just require changing one line. The `optimize` function can stay the same, but it will now be optimizing this new loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNA3JOHH2cKN"
   },
   "outputs": [],
   "source": [
    "class NeuronComputationModel(Model):\n",
    "    W1: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    "    W2: Float[Tensor, \"n_instances n_features n_hidden\"]\n",
    "    b_final: Float[Tensor, \"n_instances n_features\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: Optional[Tensor] = None,\n",
    "        importance: Optional[Tensor] = None,\n",
    "        device=device\n",
    "    ):\n",
    "        super().__init__(cfg, feature_probability, importance, device)\n",
    "\n",
    "        del self.W\n",
    "        self.W1 = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden, cfg.n_features))))\n",
    "        self.W2 = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_features, cfg.n_hidden))))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... instances features\"]\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def generate_batch(self, batch_size) -> Tensor:\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch instances features\"],\n",
    "        batch: Float[Tensor, \"batch instances features\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "tests.test_neuron_computation_model(NeuronComputationModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhSqR-gP2cKN"
   },
   "source": [
    "Once you've passed these two tests, you can run the code below to make the same visualisation as above.\n",
    "\n",
    "You should see similar patterns: with very low sparsity most/all neurons are monosemantic, but more polysemantic neurons appear as sparsity increases (until all neurons are polysemantic). Another interesting observation: in the monosemantic (or mostly monosemantic) cases, for any given feature there will be some neurons which have positive exposures to that feature and others with negative exposure. This is because some neurons are representing the value $\\operatorname{ReLU}(x_i)$ and others are representing the value of $\\operatorname{ReLU}(-x_i)$ (as discussed above, we require both of these to compute the absolute value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcZApwlf2cKN"
   },
   "outputs": [],
   "source": [
    "n_features = 100\n",
    "n_hidden = 40\n",
    "\n",
    "importance = einops.rearrange(0.8 ** t.arange(1, 1+n_features), \"feats -> () feats\")\n",
    "feature_probability = einops.rearrange(t.tensor([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001]), \"instances -> instances ()\")\n",
    "\n",
    "cfg = Config(\n",
    "    n_instances = len(feature_probability.squeeze()),\n",
    "    n_features = n_features,\n",
    "    n_hidden = n_hidden,\n",
    ")\n",
    "\n",
    "model = NeuronComputationModel(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    importance = importance,\n",
    "    feature_probability = feature_probability,\n",
    ")\n",
    "model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLhyWClo2cKN"
   },
   "outputs": [],
   "source": [
    "plot_features_in_Nd(\n",
    "    model.W1,\n",
    "    height = 800,\n",
    "    width = 1600,\n",
    "    title = f\"Neuron computation model: n_features = {n_features}, d_hidden = {n_hidden}, I<sub>i</sub> = 0.75<sup>i</sup>\",\n",
    "    subplot_titles = [f\"1 - S = {i:.3f}\" for i in feature_probability.squeeze()],\n",
    "    neuron_plot = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm-QSMc12cKN"
   },
   "source": [
    "To further confirm that this is happening, we can color the values in the bar chart discretely by feature, rather than continuously by the polysemanticity of that feature. We'll use a feature probability of 50% for this visualisation, which is high enough to make sure each neuron is monosemantic. You should find that the input weights $W_1$ form pairs of antipodal neurons (i.e. ones with positive / negative exposures to that feature direction), but both of these neurons have positive output weights $W_2$ for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSnouZ282Kef"
   },
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "n_hidden = 10\n",
    "\n",
    "importance = einops.rearrange(0.8 ** t.arange(1, 1+n_features), \"feats -> () feats\")\n",
    "\n",
    "cfg = Config(\n",
    "    n_instances = 5,\n",
    "    n_features = n_features,\n",
    "    n_hidden = n_hidden,\n",
    ")\n",
    "\n",
    "model = NeuronComputationModel(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    importance = importance,\n",
    "    feature_probability = 0.5,\n",
    ")\n",
    "model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRJtGnv_2JqU"
   },
   "outputs": [],
   "source": [
    "plot_features_in_Nd_discrete(\n",
    "    W1 = model.W1,\n",
    "    W2 = model.W2,\n",
    "    height = 600,\n",
    "    width = 1200,\n",
    "    title = f\"Neuron computation model (colored discretely, by feature)\",\n",
    "    legend_names = [f\"I<sub>{i}</sub> = {importance.squeeze()[i]:.3f}\" for i in range(n_features)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HYPbeM52cKN"
   },
   "source": [
    "## Bonus - the asymmetric superposition motif\n",
    "\n",
    "In the [corresponding section](https://transformer-circuits.pub/2022/toy_model/index.html#computation-asymmetric-motif) of Anthropic's paper, they discuss a particular quirk of this toy model in detail. Their section explains it in more detail than we will here (including some visual explanations), but we'll provide a relatively brief explanation here.\n",
    "\n",
    "> When we increase sparsity in our model & start to get superposed features, we don't always have monosemantic neurons which each calculate either $\\operatorname{ReLU}(x_i)$ or $\\operatorname{ReLU}(x_i)$ for some feature $i$. Instead, we sometimes have **asymmetric superposition, where a single neuron detects two different features $i$ and $j$, and stores these features with different magnitudes (assume the $W_1$ vector for feature $i$ is much larger). The $W_2$ vectors have flipped magnitudes (i.e. the vector for $j$ is much larger). When $i$ is present and $j$ is not, there's no problem, because the output for feature $i$ is `large * small` (correct size) and for $j$ is `small * small` (near zero). But when $j$ is present and $i$ is not, the output for feature $j$ is `small * large` (correct size) and for $i$ is `large * large` (much larger than it should be). In particular, this is bad when the sign of output for $i$ is positive. The model fixes this by repurposing another neuron to correct for the case when $j$ is present and $i$ is not. We omit the exact mechanism, but it takes advantage of the fact that the model has a ReLU at the very end, so it doesn't matter if output for a feature is very large and negative (the loss will be truncated at zero), but being large and positive is very bad.\n",
    "\n",
    "Read the linked section of the Anthropic paper for details. Can you find a set of hyperparameters (importance, sparsity values, number of features and neurons) where this behaviour is observed?\n",
    "\n",
    "Note - we recommend sticking with 5000 optimization steps or fewer. Overtraining this mdoel can cause the magnitudes of $W_1$ to collapse, and $W_2$ to get very large, which makes the plot harder to visually interpret.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (set of values I found which produced this pattern)</summary>\n",
    "\n",
    "I used `n_features=6` and `d_hidden=10` as seen in Anthropic's diagram. Feature probabilities are all $0.25$. Importances are the same as in the example case above; $I_i = 0.8^i$. Around half the instances I trained with these parameters had at least one monosemantic neuron *and* at least one pair of neurons which showed this pattern.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XAcge_U2cKN"
   },
   "outputs": [],
   "source": [
    "n_features = 6\n",
    "n_hidden = 10\n",
    "\n",
    "importance = einops.rearrange(0.8 ** t.arange(1, 1+n_features), \"feats -> () feats\")\n",
    "\n",
    "cfg = Config(\n",
    "    n_instances = 6,\n",
    "    n_features = n_features,\n",
    "    n_hidden = n_hidden,\n",
    ")\n",
    "\n",
    "model = NeuronComputationModel(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    importance = importance,\n",
    "    feature_probability = 0.25,\n",
    ")\n",
    "model.optimize(steps=5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eD2TSuQg2cKN"
   },
   "outputs": [],
   "source": [
    "plot_features_in_Nd_discrete(\n",
    "    W1 = model.W1,\n",
    "    W2 = model.W2,\n",
    "    height = 500,\n",
    "    width = 1200,\n",
    "    title = f\"Stacked neuron plots for `NeuronComputationModel` (colored discretely, by feature)\",\n",
    "    legend_names = [f\"I<sub>{i}</sub> = {importance.squeeze()[i]:.3f}\" for i in range(n_features)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Pb8p7Hh5DS8"
   },
   "source": [
    "## Summary - what have we learned?\n",
    "\n",
    "With toy models like this, it's important to make sure we take away generalizable lessons, rather than just details of the training setup.\n",
    "\n",
    "The core things to take away form this paper are:\n",
    "\n",
    "* What superposition is\n",
    "* How it varies over feature importance and sparsity\n",
    "* How it varies when we have correlated or anticorrelated features\n",
    "* The difference between neuron and bottleneck superposition (or equivalently \"computational and representational supervision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnaswYse5DS9"
   },
   "source": [
    "# (4) Feature Geometry\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8AT1cvzxAUB"
   },
   "source": [
    "> Note - this section is optional, since it goes into quite extreme detail about the specific problem setup we're using here. If you want, you can jump to the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sa5-dTkz5DS9"
   },
   "source": [
    "We've seen that superposition can allow a model to represent extra features, and that the number of extra features increases as we increase sparsity. In this section, we'll investigate this relationship in more detail, discovering an unexpected geometric story: features seem to organize themselves into geometric structures such as pentagons and tetrahedrons!\n",
    "\n",
    "The code below runs a third experiment, with all importances the same. We're first interested in the number of features the model has learned to represent. This is well represented with the squared **Frobenius norm** of the weight matrix $W$, i.e. $||W||_F^2 = \\sum_{ij}W_{ij}^2$.\n",
    "\n",
    "<details>\n",
    "<summary>Question - can you see why this is a good metric for the number of features represented?</summary>\n",
    "\n",
    "By reordering the sums, we can show that the squared Frobenius norm is the sum of the squared norms of each of the 2D embedding vectors:\n",
    "\n",
    "$$\n",
    "\\big\\|W\\big\\|_F^2 = \\sum_{j}\\big\\|W_{[:, j]}\\big\\|^2 = \\sum_j \\left(\\sum_i W_{ij}^2\\right)\n",
    "$$\n",
    "\n",
    "Each of these embedding vectors has squared norm approximately $1$ if a feature is represented, and $0$ if it isn't. So this is roughly the total number of represented features.\n",
    "</details>\n",
    "\n",
    "If you run the code below, you'll also plot the total number of \"dimensions per feature\", $m/\\big\\|W\\big\\|_F^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxByvek45DS9"
   },
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    n_features = 200,\n",
    "    n_hidden = 20,\n",
    "    n_instances = 20,\n",
    ")\n",
    "\n",
    "# For this experiment, use constant importance across features\n",
    "feature_probability = (20 ** -t.linspace(0, 1, cfg.n_instances))\n",
    "feature_probability = einops.rearrange(feature_probability, \"instances -> instances ()\")\n",
    "\n",
    "model = Model(\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    feature_probability = feature_probability,\n",
    ")\n",
    "model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJUe9ofL5DS9"
   },
   "outputs": [],
   "source": [
    "plot_feature_geometry(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FlxC9395DS9"
   },
   "source": [
    "Surprisingly, we find that this graph is \"sticky\" at $1$ and $1/2$. On inspection, the $1/2$ \"sticky point\" seems to correspond to a precise geometric arrangement where features come in \"antipodal pairs\", each being exactly the negative of the other, allowing two features to be packed into each hidden dimension. It appears that antipodal pairs are so effective that the model preferentially uses them over a wide range of the sparsity regime.\n",
    "\n",
    "It turns out that antipodal pairs are just the tip of the iceberg. Hiding underneath this curve are a number of extremely specific geometric configurations of features.\n",
    "\n",
    "How can we discover these geometric configurations? Consider the following metric, which the authors named the **dimensionality** of a feature:\n",
    "\n",
    "$$\n",
    "D_i = \\frac{\\big\\|W_i\\big\\|^2}{\\sum_{j} \\big( \\hat{W_i} \\cdot W_j \\big)^2}\n",
    "$$\n",
    "\n",
    "Intuitively, this is a measure of what \"fraction of a dimension\" a specific feature gets. Let's try and get a few intuitions for this metric:\n",
    "\n",
    "* It's never less than zero.\n",
    "    * It's equal to zero if and only if the vector is the zero vector, i.e. the feature isn't represented.\n",
    "* It's never greater than one (because when $j = i$, the term in the denominator sum is equal to the numerator).\n",
    "    * It's equal to one if and only if the $i$-th feature vector $W_i$ is orthogonal to all other features (because then $j=i$ is the only term in the denominator sum).\n",
    "    * Intuitively, in this case the feature has an entire dimension to itself.\n",
    "* If there are $k$ features which are all parallel to each other, and orthogonal to all others, then they \"share\" the dimensionality equally, i.e. $D_i = 1/k$ for each of them.\n",
    "* The sum of all $D_i$ can't be greater than the total number of features $m$, with equality if and only if all the vectors are orthogonal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seKqykCl5DS-"
   },
   "source": [
    "### Exercise - compute dimensionality\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴🔴⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 10-15 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Remember, $W$ has shape `(n_instances, n_hidden, n_features)`. The vectors $W_i$ refer to the feature vectors (i.e. they have length `n_hidden`), and you should broadcast your calculations over the `n_instances` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UML7S4Ap5DS-"
   },
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def compute_dimensionality(\n",
    "    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    ") -> Float[Tensor, \"n_instances n_features\"]:\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "tests.test_compute_dimensionality(compute_dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZL533B25DS-"
   },
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "def compute_dimensionality(\n",
    "    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    ") -> Float[Tensor, \"n_instances n_features\"]:\n",
    "    # SOLUTION\n",
    "    # Compute numerator terms\n",
    "    W_norms = W.norm(dim=1, keepdim=True)\n",
    "    numerator = W_norms.squeeze() ** 2\n",
    "\n",
    "    # Compute denominator terms\n",
    "    W_normalized = W / W_norms\n",
    "    # t.clamp(W_norms, 1e-6, float(\"inf\"))\n",
    "    denominator = einops.einsum(W_normalized, W, \"i h f1, i h f2 -> i f1 f2\").pow(2).sum(-1)\n",
    "\n",
    "    return numerator / denominator\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nht7vgIH5DS-"
   },
   "source": [
    "\n",
    "The code below plots the fractions of dimensions, as a function of sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KomeBaC5DS-"
   },
   "outputs": [],
   "source": [
    "W = model.W.detach()\n",
    "dim_fracs = compute_dimensionality(W)\n",
    "\n",
    "plot_feature_geometry(model, dim_fracs=dim_fracs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM-DIe785DS_"
   },
   "source": [
    "What's going on here? It turns out that the model likes to create specific weight geometries and kind of jumps between the different configurations.\n",
    "\n",
    "The moral? Superposition is very hard to pin down! There are many points between a dimensionality of 0 (not learning a feature) and 1 (dedicating a dimension to a feature). As an analogy, we often think of water as only having three phases: ice, water and steam. But this is a simplification: there are actually many phases of ice, often corresponding to different crystal structures (eg. hexagonal vs cubic ice). In a vaguely similar way, neural network features seem to also have many other phases within the general category of \"superposition.\"\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/grid_all.png\" width=\"900\">\n",
    "\n",
    "Note that we should take care not to read too much significance into these results. A lot of it depends delicately on the details of our experimental setup (e.g. we used $W^T W$, a positive semidefinite matrix, and there's a correspondence between low-dimensional symmetric pos-semidef matrices like these and the kinds of polytopes that we've seen in the plots above). But hopefully this has given you a sense of the relevant considerations when it comes to packing features into fewer dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UygCcgPP2cKP"
   },
   "source": [
    "# (5) Deep Double Descent & Superposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWsmcIkb2cKP"
   },
   "source": [
    "For a final set of exercises in this set, we'll look at the Anthropic paper [Superposition, Memorization and Deep Double Descent](https://transformer-circuits.pub/2023/toy-double-descent/index.html). This paper ties the phenomena of [double descent](https://openai.com/research/deep-double-descent) to models of superposition. The theory posed by this paper goes roughly as follows:\n",
    "\n",
    "* Initially, the model learns a **memorising solution** where datapoints are represented in superposition. This doesn't generalize, so we get low training loss but high test loss.\n",
    "* Later, the model learns a **generalizing solution** where features are learned and represented in superposition. This generalizes, so we get low training loss and low test loss.\n",
    "* The spike in loss between these two happens when the model transitions between the memorising and generalizing solutions.\n",
    "\n",
    "What does it mean to represent datapoints in superposition? If you've done the exercises on correlated / anticorrelated features in an earlier section, you'll know that anticorrelated features are easier to represent in superposition because they don't interfere with each other. This is especially true if features aren't just anticorrelated but are **mutually exclusive**. From the Anthropic paper:\n",
    "\n",
    "> Consider the case of a language model which verbatim memorizes text. How can it do this? One naive idea is that it might use neurons to create a lookup table mapping sequences to arbitrary continuations. For every sequence of tokens it wishes to memorize, it could dedicate one neuron to detecting that sequence, and then implement arbitrary behavior when it fires. The problem with this approach is that it's extremely inefficient – but it seems like a perfect candidate for superposition, since each case is mutually exclusive and can't interfere.\n",
    "\n",
    "We'll study this theory in the context of a toy model. Specifically, we'll use the toy model that we worked with in the first section of this paper, but we'll train it in a very particular way: by generating a random batch of data, and then using that same batch for the entire training process. We'll see what happens when the batch sizes change, but the number of features change. According to our theory, the model should represent datapoints in superposition when the batch size is smaller than the number of features, and it should represent features in superposition when the batch size is larger than the number of features.\n",
    "\n",
    "Rather than giving you a set of exercises to complete, we're leaving this section open-ended. You should consider it more as a paper replication than a set of structured exercises. However, we will give you a few tips:\n",
    "\n",
    "* Rather than using the Adam optimizer, the paper recommends AdamW, with a default weight decay of `WEIGHT_DECAY = 1e-2`.\n",
    "    * Weight decay constrains the norm of weights, so that they don't grow too large. With no weight decay, we could in theory memorize an arbitrarily large number of datapoints and represent them evenly spaced around the unit circle; then we can perfectly reconstruct them as long as we have a large enough weight vector to project them onto.\n",
    "* The paper recommends a learning rate consisting of a linear warmup up to `NUM_WARMUP_STEPS = 2500` (i.e. we increase the learning rate linearly from zero up to `LEARNING_RATE = 1e-3`), followed by cosine decay until the end of training at `NUM_BATCH_UPDATES = 50_000`.\n",
    "* The paper recommends using a sparsity of 0.999 for the features, and 10,000 features total. However, we recommend instead using `SPARSITY = 0.99` and `N_FEATURES = 1000` (following the replication by Marius Hobbhahn). This will cause our model to train faster, while still observing fundamentally the same patterns.\n",
    "* When generating the batch of data, you should normalize it (so each vector for a given batch index & instance has unit norm). The rest of the data generation process should be the same as in the first section of this notebook.\n",
    "* Technically you only need one instance. However, we recommend using a few (e.g. 5-10) so you can pick the instance with lowest loss at the end of training. This is because (thanks to our best frend randomness) not all instances will necessarily learn the optimal solution. In our implementation (code below), we rewrite the `optimize` function to return `(batch_inst, W_inst)` at the end, where `batch_inst` is the batch which had the lowest loss by the end of training, and `W_inst` are the learned weights for that same instance. This is precisely the data you'll need to make the 2D feature plots featured in the paper.\n",
    "* You can repurpose the function to calculate **dimensionality** from the section on feature geometry. See the paper for a discussion of a generalized dimensionality function, which doesn't just measure dimensionality of features, but also of datapoints.\n",
    "\n",
    "To get you started, here are some constants which you might find useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ae8neJp2cKQ"
   },
   "outputs": [],
   "source": [
    "NUM_WARMUP_STEPS = 2500\n",
    "NUM_BATCH_UPDATES = 50_000\n",
    "\n",
    "WEIGHT_DECAY = 1e-2\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "BATCH_SIZES = [3, 5, 6, 8, 10, 15, 30, 50, 100, 200, 500, 1000, 2000]\n",
    "\n",
    "N_FEATURES = 1000\n",
    "N_INSTANCES = 5\n",
    "N_HIDDEN = 2\n",
    "SPARSITY = 0.99\n",
    "FEATURE_PROBABILITY = 1 - SPARSITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9R55X4l2cKQ"
   },
   "source": [
    "Also, if you want some help with the visualisation, the code below will produce the 2D feature visualisations like those found at the bottom of [this figure](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/fig-2d.png), for all batch sizes stacked horizontally, assuming:\n",
    "\n",
    "* `features_list` is a list of detached `W`-matrices for single instances, i.e. each has shape `(2, n_features)` (these will be used to produce the blue plots on the first row)\n",
    "* `data_list` is a list of the projections of our batch of data onto the hidden directions of that same instance, i.e. each has shape `(2, batch_size)` (these will be used to produce the red plots on the second row)\n",
    "\n",
    "A demonstration is given below (obviously the values are meaningless, they've just been randomly generated for the purposes of the visualisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRelJ2HV2cKQ"
   },
   "outputs": [],
   "source": [
    "features_list = [t.randn(size=(2, 100)) for _ in BATCH_SIZES]\n",
    "hidden_representations_list = [t.randn(size=(2, batch_size)) for batch_size in BATCH_SIZES]\n",
    "\n",
    "plot_features_in_2d(\n",
    "    features_list + hidden_representations_list,\n",
    "    colors = [[\"blue\"] for _ in range(len(BATCH_SIZES))] + [[\"red\"] for _ in range(len(BATCH_SIZES))],\n",
    "    title = \"Double Descent & Superposition (num features = 100)\",\n",
    "    subplot_titles = [f\"Features (batch={bs})\" for bs in BATCH_SIZES] + [f\"Data (batch={bs})\" for bs in BATCH_SIZES],\n",
    "    n_rows = 2,\n",
    "    adjustable_limits = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tr_McpSd2cKQ"
   },
   "source": [
    "You can click on the dropdown below to see a full replication of these results (although it's currently missing the loss curve - if you want an easier challenge than replicating the results from scratch you could try adding this in yourself, starting from the code below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZlA3raf2cKQ"
   },
   "source": [
    "<details>\n",
    "<summary>Implementation</summary>\n",
    "\n",
    "```python\n",
    "NUM_WARMUP_STEPS = 2500\n",
    "NUM_BATCH_UPDATES = 50_000\n",
    "# EVAL_N_DATAPOINTS = 1_000\n",
    "\n",
    "WEIGHT_DECAY = 1e-2\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "BATCH_SIZES = [3, 4, 5, 6, 8, 10, 15, 20, 30, 50, 100, 200, 300, 500, 1000, 2000, 3000]\n",
    "# BATCH_SIZES = [3, 5, 6, 8, 10, 15, 30, 50, 100, 200, 500, 1000, 2000][::2]\n",
    "\n",
    "N_FEATURES = 1000\n",
    "N_INSTANCES = 10\n",
    "N_HIDDEN = 2\n",
    "SPARSITY = 0.99\n",
    "FEATURE_PROBABILITY = 1 - SPARSITY\n",
    "\n",
    "\n",
    "def linear_lr(step, steps):\n",
    "    '''\n",
    "    Decays linearly from 1 to 0.\n",
    "    '''\n",
    "    return (1 - (step / steps))\n",
    "\n",
    "def linear_warmup_lr(step, steps):\n",
    "    '''\n",
    "    Increases linearly from 0 to 1.\n",
    "    '''\n",
    "    return step / steps\n",
    "\n",
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "def cosine_decay_lr(step, steps):\n",
    "    return np.cos(0.5 * np.pi * step / (steps - 1))\n",
    "\n",
    "def anthropic_lr(step, steps):\n",
    "    '''\n",
    "    As per the description in the paper: 2500 step linear warmup, followed by\n",
    "    cosine decay to zero.\n",
    "    '''\n",
    "    if step < NUM_WARMUP_STEPS:\n",
    "        return linear_warmup_lr(step, NUM_WARMUP_STEPS)\n",
    "    else:\n",
    "        return cosine_decay_lr(step - NUM_WARMUP_STEPS, steps - NUM_WARMUP_STEPS)\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Same as TMS, we're leaving in the \"n_instances\" argument for more possible\n",
    "    flexibility later (even though I don't think I'll use it).\n",
    "    \"\"\"\n",
    "    n_instances: int = 1\n",
    "    n_features: int = N_FEATURES\n",
    "    n_hidden: int = N_HIDDEN\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    "    b_final: Float[Tensor, \"n_instances n_features\"]\n",
    "    # Our linear map (ignoring n_instances) is x -> ReLU(W.T @ W @ x + b_final)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: Optional[Union[Tensor, float]] = 1 - SPARSITY,\n",
    "        importance: Optional[Union[Tensor, float]] = None,\n",
    "        device = device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if feature_probability is None: feature_probability = t.ones(())\n",
    "        elif isinstance(feature_probability, float): feature_probability = t.ones(()) * feature_probability\n",
    "        self.feature_probability = feature_probability.to(device).broadcast_to((cfg.n_instances, cfg.n_features))\n",
    "        self.sparsity = 1 - self.feature_probability\n",
    "\n",
    "        if importance is None: importance = t.ones(())\n",
    "        elif isinstance(importance, float): importance = t.ones(()) * importance\n",
    "        self.importance = importance.to(device).broadcast_to((cfg.n_instances, cfg.n_features))\n",
    "\n",
    "        self.W = nn.Parameter(t.empty((cfg.n_instances, cfg.n_hidden, cfg.n_features), device=device))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        self.b_final = nn.Parameter(t.zeros((cfg.n_instances, cfg.n_features), device=device))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def dimensionality(cls, data: Float[Tensor, \"... batch d_hidden\"]) -> Float[Tensor, \"... batch\"]:\n",
    "        '''\n",
    "        Calculates dimensionalities of data. Assumes data is of shape ... batch d_hidden, i.e. if it's 2D then\n",
    "        it's a batch of vectors of length `d_hidden` and we return the dimensionality as a 1D tensor of length\n",
    "        `batch`. If it has more dimensions at the start, we assume this means separate calculations for each\n",
    "        of these dimensions (i.e. they are independent batches of vectors).\n",
    "        '''\n",
    "        # Compute the norms of each vector (this will be the numerator)\n",
    "        squared_norms = einops.reduce(\n",
    "            data.pow(2),\n",
    "            \"... batch d_hidden -> ... batch\",\n",
    "            \"sum\",\n",
    "        )\n",
    "        # Compute the denominator (i.e. get the dotproduct then sum over j)\n",
    "        data_normed = data / data.norm(dim=-1, keepdim=True)\n",
    "        interference = einops.einsum(\n",
    "            data_normed, data,\n",
    "            \"... batch_i d_hidden, ... batch_j d_hidden -> ... batch_i batch_j\",\n",
    "        )\n",
    "        polysemanticity = einops.reduce(\n",
    "            interference.pow(2),\n",
    "            \"... batch_i batch_j -> ... batch_i\",\n",
    "            \"sum\",\n",
    "        )\n",
    "        assert squared_norms.shape == polysemanticity.shape\n",
    "\n",
    "        return squared_norms / polysemanticity\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: Float[Tensor, \"... instances features\"],\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "        hidden = einops.einsum(\n",
    "           features, self.W,\n",
    "           \"... instances features, instances hidden features -> ... instances hidden\"\n",
    "        )\n",
    "        out = einops.einsum(\n",
    "            hidden, self.W,\n",
    "            \"... instances hidden, instances hidden features -> ... instances features\"\n",
    "        )\n",
    "        return F.relu(out + self.b_final)\n",
    "\n",
    "\n",
    "    def generate_batch(self, batch_size: int) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "\n",
    "        # Get values of features pre-choosing some of them to be zero\n",
    "        feat = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device=self.W.device) # [batch instances features]\n",
    "\n",
    "        # Choose which features to be zero\n",
    "        feat_seeds = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device=self.W.device) # [batch instances features]\n",
    "        feat_is_present = feat_seeds <= self.feature_probability\n",
    "\n",
    "        # Zero out the features\n",
    "        batch = t.where(feat_is_present, feat, t.zeros((), device=self.W.device))\n",
    "\n",
    "        # Normalize the batch (i.e. so each vector for a particular batch & instance has norm 1)\n",
    "        # (need to be careful about vectors with norm zero)\n",
    "        norms = batch.norm(dim=-1, keepdim=True)\n",
    "        norms = t.where(norms.abs() < 1e-6, t.ones_like(norms), norms)\n",
    "        batch_normed = batch / norms\n",
    "\n",
    "        return batch_normed\n",
    "\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch instances features\"],\n",
    "        batch: Float[Tensor, \"batch instances features\"],\n",
    "        loss_per_instance: bool = False,\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        \n",
    "        error = self.importance * ((batch - out) ** 2)\n",
    "        loss = einops.reduce(error, 'batch instances features -> instances', 'mean')\n",
    "        return loss if loss_per_instance else loss.sum()\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        num_batch_updates: int = NUM_BATCH_UPDATES,\n",
    "        log_freq: int = 100,\n",
    "        lr: float = LEARNING_RATE,\n",
    "        lr_scale: Callable[[int, int], float] = anthropic_lr,\n",
    "        weight_decay: float = WEIGHT_DECAY,\n",
    "    ):\n",
    "        optimizer = t.optim.AdamW(list(self.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        progress_bar = tqdm(range(num_batch_updates))\n",
    "\n",
    "        # Same batch for each step\n",
    "        batch = self.generate_batch(batch_size) # [batch_size instances n_features]\n",
    "        \n",
    "        for step in progress_bar:\n",
    "\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, num_batch_updates)\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = step_lr\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            out = self.forward(batch)\n",
    "            loss = self.calculate_loss(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display progress bar\n",
    "            if (step % log_freq == 0) or (step + 1 == num_batch_updates):\n",
    "                progress_bar.set_postfix(loss=loss.item()/self.cfg.n_instances, lr=step_lr)\n",
    "\n",
    "        # Generate one final batch to compute the loss (we want only the best instance!)\n",
    "        with t.inference_mode():\n",
    "            out = self.forward(batch)\n",
    "            loss = self.calculate_loss(out, batch, loss_per_instance=True)\n",
    "            best_instance = loss.argmin()\n",
    "            print(f\"Best instance = #{best_instance}, with loss {loss[best_instance].item()}\")\n",
    "\n",
    "        return batch[:, best_instance], self.W[best_instance].detach()\n",
    "```\n",
    "\n",
    "Now, actually generating the data:\n",
    "\n",
    "```python\n",
    "features_list = []\n",
    "hidden_representations_list = []\n",
    "\n",
    "for batch_size in tqdm(BATCH_SIZES):\n",
    "\n",
    "    # Clear memory between runs\n",
    "    t.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Define our model\n",
    "    cfg = Config(n_features=N_FEATURES, n_instances=N_INSTANCES)\n",
    "    model = Model(cfg, feature_probability=FEATURE_PROBABILITY).to(device)\n",
    "\n",
    "    # Optimize, and return the best batch & weight matrix\n",
    "    batch_inst, W_inst = model.optimize(batch_size=batch_size, num_batch_updates=15_000)\n",
    "\n",
    "    # Calculate the hidden feature representations, and add both this and weight matrix to our lists of data\n",
    "    with t.inference_mode():\n",
    "        hidden = einops.einsum(batch_inst, W_inst, \"batch features, hidden features -> hidden batch\")\n",
    "    features_list.append(W_inst.cpu())\n",
    "    hidden_representations_list.append(hidden.cpu())\n",
    "```\n",
    "\n",
    "Visualising the 2D feature plots:\n",
    "\n",
    "```python\n",
    "plot_features_in_2d(\n",
    "    features_list + hidden_representations_list,\n",
    "    colors = [[\"blue\"] for _ in range(len(BATCH_SIZES))] + [[\"red\"] for _ in range(len(BATCH_SIZES))],\n",
    "    title = \"Double Descent & Superposition (num features = 1000)\",\n",
    "    subplot_titles = [f\"Features (batch={bs})\" for bs in BATCH_SIZES] + [f\"Data (batch={bs})\" for bs in BATCH_SIZES],\n",
    "    n_rows = 2,\n",
    "    adjustable_limits = True,\n",
    ")\n",
    "```\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ddd_fig1.png\" width=\"1400\">\n",
    "\n",
    "and the dimensionality of features / data:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "df_data = {\"Batch size\": [], \"Dimensionality\": [], \"Data\": []}\n",
    "\n",
    "for batch_size, model_W, hidden in zip(BATCH_SIZES, features_list, hidden_representations_list):\n",
    "\n",
    "    # Get x-axis data (batch size), and color (blue or red)\n",
    "    df_data[\"Batch size\"].extend([batch_size] * (N_FEATURES + batch_size))\n",
    "    df_data[\"Data\"].extend([\"features\"] * N_FEATURES + [\"hidden\"] * batch_size)\n",
    "\n",
    "    # Calculate dimensionality of model.W[inst].T, which has shape [d_hidden=2 N_FEATURES]\n",
    "    feature_dim = Model.dimensionality(model_W.T)\n",
    "    assert feature_dim.shape == (N_FEATURES,)\n",
    "    # Calculate dimensionality of model's batch data hidden representation. This has shape [d_hidden=2 batch_size]\n",
    "    data_dim = Model.dimensionality(hidden.T)\n",
    "    assert data_dim.shape == (batch_size,)\n",
    "    # Add them both to the data\n",
    "    df_data[\"Dimensionality\"].extend(feature_dim.tolist() + data_dim.tolist())\n",
    "\n",
    "\n",
    "df = pd.DataFrame(df_data)\n",
    "delta = 0.01\n",
    "fig = (\n",
    "    px.strip(\n",
    "        df, x='Batch size', y='Dimensionality', color=\"Data\",\n",
    "        color_discrete_sequence=[f\"rgba(0,0,255,0.3)\", f\"rgba(255,0,0,0.3)\"],\n",
    "        log_x=True, template=\"simple_white\", width=1000, height=600,\n",
    "        title=\"Dimensionality of features & hidden representation of training examples\"\n",
    "    )\n",
    "    .update_traces(marker=dict(opacity=0.5))\n",
    "    .update_layout(\n",
    "        xaxis=dict(range=[math.log10(1.5), math.log10(5000)], tickmode='array', tickvals=BATCH_SIZES),\n",
    "        yaxis=dict(range=[-0.05, 1.0])\n",
    "    )\n",
    "    .add_vrect(x0=1, x1=(1-delta) * (100*200)**0.5-delta, fillcolor=\"#ddd\", opacity=0.5, layer=\"below\", line_width=0)\n",
    "    .add_vrect(x0=(1+delta) * (100*200)**0.5+delta, x1=(1-delta) * (500*1000)**0.5, fillcolor=\"#ccc\", opacity=0.5, layer=\"below\", line_width=0)\n",
    "    .add_vrect(x0=(1+delta) * (500*1000)**0.5+delta, x1=10_000, fillcolor=\"#bbb\", opacity=0.5, layer=\"below\", line_width=0)\n",
    "    .add_scatter(x=BATCH_SIZES, y=[2 / b for b in BATCH_SIZES], mode=\"lines\", line=dict(shape=\"spline\", dash=\"dot\", color=\"#333\", width=1), name=\"d_hidden / batch_size\")\n",
    ")\n",
    "\n",
    "fig.show(config=dict(staticPlot=True))\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/ddd_fig2.png\" width=\"800\">\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MD88v4Zvw-r"
   },
   "source": [
    "# (6) Sparse Autoencoders in Toy Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVKobnx_vw-r"
   },
   "source": [
    "We now move on to sparse autoencoders, a recent line of work that has been explored by Anthropic in their [recent paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html), and is currently one of the most interesting areas of research in mechanistic interpretability.\n",
    "\n",
    "In the following set of exercises, you will:\n",
    "\n",
    "- Build your own sparse autoencoder, writing its architecture & loss function,\n",
    "- Train your SAE on the hidden activations of the `Model` class which you defined earlier (note the difference between this and the Anthropic paper's setup, since the latter trained SAEs on the MLP layer, whereas we're training it on a non-privileged basis),\n",
    "- Extract the features from your SAE, and verify that these are the same as your model's learned features.\n",
    "\n",
    "### Reading\n",
    "\n",
    "You should read Anthropic's dictionary learning paper (linked above): the introduction and first section (problem setup) up to and including the \"Sparse Autoencoder Setup\" section. Make sure you can answer at least the following questions:\n",
    "\n",
    "- What is an autoencoder, and what is it trained to do?\n",
    "- Why is the hidden dimension of our autoencoder larger than the number of activations, when we train an SAE on an MLP layer?\n",
    "- Why does the $L_1$ penalty encourage sparsity? (This isn't specifically mentioned in this paper, but it's an important thing to understand.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ8uXi56vw-r"
   },
   "source": [
    "### Problem setup\n",
    "\n",
    "Recall the formulation of our previous model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h &= W x \\\\\n",
    "x' &= \\operatorname{ReLU}(W^T h + b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We're going to train our autoencoder to just take in the hidden state activations $h$, map them to a larger (overcomplete) hidden state $z$, then reconstruct the original hidden state $h$ from $z$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z &= \\operatorname{ReLU}(W_{enc}(h - b_{dec}) + b_{enc}) \\\\\n",
    "h' &= W_{dec}z + b_{dec}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note the choice to have a different encoder and decoder weight matrix, rather than having them tied - we'll discuss this more later.\n",
    "\n",
    "It's important not to get confused between the autoencoder and model's notation. Remember - the model takes in features $x$, maps them to **lower-dimensional** vectors $h$, and then reconstructs them as $x'$. The autoencoder takes in these hidden states $h$, maps them to a **higher-dimensional but sparse** vector $z$, and then reconstructs them as $h'$. Our hope is that the elements of $z$ correspond to the features of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phZwW22y2cKQ"
   },
   "source": [
    "### Notation\n",
    "\n",
    "Sometimes each of the hidden activations of the autoencoder are called **neurons**, sometimes they are called **features**. Because we're training our autoencoder on a model without a privileged basis in this section, we can safely refer to them as neurons without potentially getting them confused with the model's hidden activations (when we move onto the next section, we'll be explicit about which one we mean when we say \"neurons\"). As for \"features\", we'll usually use **autoencoder features** to refer to the features learned by the autoencoder (which are not necessarily the same as the model's features), and **features** or **model features** to refer to the features of our model's data generation process, which we're trying to recover.\n",
    "\n",
    "The notation we'll use in this section is as follows:\n",
    "\n",
    "- `n_features` = number of features of your data generation process (this is the same as `n_features` from the last section).\n",
    "- `n_hidden` = number of hidden dimensions of your model (this is the same as `n_hidden` from the last section).\n",
    "- `n_input_ae` = number of input dimensions of your **autoencoder**. This is the same as `n_hidden` (because we feed our autoencoder batches of hidden-state activations from our model), but we've named it differently to avoid confusion with `n_hidden_ae`.\n",
    "- `n_hidden_ae` = number of hidden dimensions / neurons / features of your **autoencoder**. We require `n_hidden_ae >= n_features` for our autoencoder to have a chance of reconstructing all the features. Usually we'll have `n_hidden_ae == n_features` in this section.\n",
    "\n",
    "<details>\n",
    "<summary>Question - in the formulas above (in the \"Problem setup\" section), what are the shapes of x, x', z, h, and h' ?</summary>\n",
    "\n",
    "Ignoring batch and instance dimensions:\n",
    "\n",
    "- `x` and `x'` are vectors of shape `(n_features,)`\n",
    "- `z` is a vector of shape `(n_hidden_ae,)`, which will usually be equal to or larger than `n_features` (so that our autoencoder is overcomplete)\n",
    "- `h` and `h'` are vectors of shape `(n_input_ae,)`, also equal to `(n_hidden,)`\n",
    "\n",
    "Including batch and instance dimensions, all shapes `(d,)` above turn into `(batch_size, n_instances, d)`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNwpo1vavw-r"
   },
   "source": [
    "### Exercise - define your SAE\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴🔴⚪\n",
    "Importance: 🔵🔵🔵🔵🔵\n",
    "\n",
    "You should spend up to 25-45 minutes on this exercise.\n",
    "```\n",
    "\n",
    "We've provided an `AEConfig` class below. Its arguments are `n_instances` (this means the same as it does in your `Model` class), `n_hidden` (which is the size of the model's hidden layer, i.e. the things which will be the input into your autoencoder), and `n_hidden_autoencoder` (which is the size of your **AutoEncoder's** hidden layer). Usually, this will be the same as the `n_features` argument of your model, since that's the number of features we're trying to learn.\n",
    "\n",
    "You should fill in the `AutoEncoder` class methods `__init__` and `forward`, so that:\n",
    "\n",
    "### `__init__`\n",
    "\n",
    "- You have weight matrices `W_dec` and `W_enc`, as well as biases `b_dec` and `b_enc`. The biases can be initialized at zero, and the weights can be initialized with xavier normal initialization like they were for your model.\n",
    "    - We've given you the type signatures for these classes, so you know what shape they should be.\n",
    "    - Remember to move your parameters to the correct device! (You can copy the code from `Model` class earlier.)\n",
    "\n",
    "### `forward`\n",
    "\n",
    "- You should calculate the autoencoder's hidden state activations as $z = \\operatorname{ReLU}(W_{enc}(h - b_{dec}) + b_{enc})$, and then reconstruct the output as $h' = W_{dec}z + b_{dec}$.\n",
    "\n",
    "- This function should return the following 5 arguments, in order:\n",
    "    - `l1_loss`, which is the absolute values of post-ReLU activations $z$, **summed** over the hidden dimension `n_hidden_ae`. This should have shape `(batch_size, n_instances)`.\n",
    "    - `l2_loss`, which is the reconstruction loss between $h$ and $h'$ (i.e. the squared differences between elements of this vector, **averaged** over the dimensionality of the vectors `n_input_ae`). This should have shape `(batch_size, n_instances)`.\n",
    "    - `loss`, which is the sum of the $L_1$ and $L_2$ losses (i.e. a scalar).\n",
    "        - Note, we should sum over the `n_instances` dimension (because each instance should be training at the same rate), but we should take the mean over the `batch_size` dimension.\n",
    "        - Also, the `l1_loss` should be multiplied by the `config.l1_coeff` parameter before adding the two scalars together.\n",
    "    - `acts`, which are the hidden state activations $z$. This should have shape `(batch_size, n_instances, n_hidden_ae)`.\n",
    "    - `h_reconstructed`, which are the reconstructed inputs $h'$. This should have shape `(batch_size, n_instances, n_input_ae)`.\n",
    "\n",
    "<details>\n",
    "<summary>Question - why do you think we sum over the hidden dimension when computing <code>l1_loss</code>, but average over the hidden dimension for <code>l2_loss</code> ?</summary>\n",
    "\n",
    "Suppose we took the average of $L_1$ loss over the `n_hidden_ae` dimension; we'll create a thought experiment to show why this would be bad.\n",
    "\n",
    "Consider a single feature, in an autoencoder with a fixed `n_input_ae` but a variable number of features `n_hidden_ae`. If we double the number of features, then any change in that feature will affect the $L_2$ loss by the same amount. But the $L_1$ loss would be affected by half that amount, since we're averaging the $L_1$ norm over all features. So the more features we have in our model, the less any given feature will be encouraged to be sparse. In the limit, feature sparsity basically won't matter at all; only reconstruction loss will matter.\n",
    "\n",
    "On the other hand, if we sum $L_1$ loss over the `n_hidden_ae` dimension, then in this thought experiment, the effect of a change in any given feature is still the same for $L_1$ and $L_2$ loss, so we don't have this problem.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "Ignore the `resample_neurons` method for now, we'll discuss it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1JctT2Pvw-r"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AutoEncoderConfig:\n",
    "    n_instances: int\n",
    "    n_input_ae: int\n",
    "    n_hidden_ae: int\n",
    "    l1_coeff: float = 0.5\n",
    "    tied_weights: bool = False\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    W_enc: Float[Tensor, \"n_instances n_input_ae n_hidden_ae\"]\n",
    "    W_dec: Float[Tensor, \"n_instances n_hidden_ae n_input_ae\"]\n",
    "    b_enc: Float[Tensor, \"n_instances n_hidden_ae\"]\n",
    "    b_dec: Float[Tensor, \"n_instances n_input_ae\"]\n",
    "\n",
    "    def __init__(self, cfg: AutoEncoderConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_enc = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_input_ae, cfg.n_hidden_ae))))\n",
    "        if not(cfg.tied_weights):\n",
    "            self.W_dec = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden_ae, cfg.n_input_ae))))\n",
    "        self.b_enc = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_hidden_ae))\n",
    "        self.b_dec = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_input_ae))\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, h: Float[Tensor, \"batch_size n_instances n_hidden\"]):\n",
    "\n",
    "        # Compute activations\n",
    "        h_cent = h - self.b_dec\n",
    "        acts = einops.einsum(\n",
    "            h_cent, self.W_enc,\n",
    "            \"batch_size n_instances n_input_ae, n_instances n_input_ae n_hidden_ae -> batch_size n_instances n_hidden_ae\"\n",
    "        )\n",
    "        acts = F.relu(acts + self.b_enc)\n",
    "\n",
    "        # Compute reconstructed input\n",
    "        h_reconstructed = einops.einsum(\n",
    "            acts, (self.W_enc.transpose(-1, -2) if self.cfg.tied_weights else self.W_dec),\n",
    "            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n",
    "        ) + self.b_dec\n",
    "\n",
    "        # Compute loss, return values\n",
    "        l2_loss = (h_reconstructed - h).pow(2).mean(-1) # shape [batch_size n_instances]\n",
    "        l1_loss = acts.abs().sum(-1) # shape [batch_size n_instances]\n",
    "        loss = (self.cfg.l1_coeff * l1_loss + l2_loss).mean(0).sum() # scalar\n",
    "\n",
    "        return l1_loss, l2_loss, loss, acts, h_reconstructed\n",
    "\n",
    "\n",
    "    @t.no_grad()\n",
    "    def normalize_decoder(self) -> None:\n",
    "        '''\n",
    "        Normalizes the decoder weights to have unit norm. If using tied weights, we we assume W_enc is used for both.\n",
    "        '''\n",
    "        if self.cfg.tied_weights:\n",
    "            self.W_enc.data = self.W_enc.data / self.W_enc.data.norm(dim=1, keepdim=True)\n",
    "        else:\n",
    "            self.W_dec.data = self.W_dec.data / self.W_dec.data.norm(dim=2, keepdim=True)\n",
    "\n",
    "\n",
    "    @t.no_grad()\n",
    "    def resample_neurons(\n",
    "        self,\n",
    "        h: Float[Tensor, \"batch_size n_instances n_hidden\"],\n",
    "        frac_active_in_window: Float[Tensor, \"window n_instances n_hidden_ae\"],\n",
    "        neuron_resample_scale: float,\n",
    "    ) -> None:\n",
    "        '''\n",
    "        Resamples neurons that have been dead for `dead_neuron_window` steps, according to `frac_active`.\n",
    "        '''\n",
    "        pass # See below for a solution to this function\n",
    "\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        model: Model,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 100,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "        neuron_resample_window: Optional[int] = None,\n",
    "        dead_neuron_window: Optional[int] = None,\n",
    "        neuron_resample_scale: float = 0.2,\n",
    "    ):\n",
    "        '''\n",
    "        Optimizes the autoencoder using the given hyperparameters.\n",
    "\n",
    "        This function should take a trained model as input.\n",
    "        '''\n",
    "        if neuron_resample_window is not None:\n",
    "            assert (dead_neuron_window is not None) and (dead_neuron_window < neuron_resample_window)\n",
    "\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "        frac_active_list = []\n",
    "        progress_bar = tqdm(range(steps))\n",
    "\n",
    "        # Create lists to store data we'll eventually be plotting\n",
    "        data_log = {\"W_enc\": [], \"W_dec\": [], \"colors\": [], \"titles\": [], \"frac_active\": []}\n",
    "        colors = None\n",
    "        title = \"no resampling yet\"\n",
    "\n",
    "        for step in progress_bar:\n",
    "\n",
    "            # Normalize the decoder weights before each optimization step\n",
    "            self.normalize_decoder()\n",
    "\n",
    "            # Resample dead neurons\n",
    "            if (neuron_resample_window is not None) and ((step + 1) % neuron_resample_window == 0):\n",
    "                # Get the fraction of neurons active in the previous window\n",
    "                frac_active_in_window = t.stack(frac_active_list[-neuron_resample_window:], dim=0)\n",
    "                # Compute batch of hidden activations which we'll use in resampling\n",
    "                batch = model.generate_batch(batch_size)\n",
    "                h = einops.einsum(batch, model.W, \"batch_size instances features, instances hidden features -> batch_size instances hidden\")\n",
    "                # Resample\n",
    "                colors, title = self.resample_neurons(h, frac_active_in_window, neuron_resample_scale)\n",
    "\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] = step_lr\n",
    "\n",
    "            # Get a batch of hidden activations from the model\n",
    "            with t.inference_mode():\n",
    "                features = model.generate_batch(batch_size)\n",
    "                h = einops.einsum(features, model.W, \"... instances features, instances hidden features -> ... instances hidden\")\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            l1_loss, l2_loss, loss, acts, _ = self.forward(h)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate the sparsities, and add it to a list\n",
    "            frac_active = einops.reduce((acts.abs() > 1e-8).float(), \"batch_size instances hidden_ae -> instances hidden_ae\", \"mean\")\n",
    "            frac_active_list.append(frac_active)\n",
    "\n",
    "            # Display progress bar, and append new values for plotting\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(l1_loss=self.cfg.l1_coeff * l1_loss.mean(0).sum().item(), l2_loss=l2_loss.mean(0).sum().item(), lr=step_lr)\n",
    "                data_log[\"W_enc\"].append(self.W_enc.detach().cpu())\n",
    "                data_log[\"W_dec\"].append(self.W_dec.detach().cpu())\n",
    "                data_log[\"colors\"].append(colors)\n",
    "                data_log[\"titles\"].append(f\"Step {step}/{steps}: {title}\")\n",
    "                data_log[\"frac_active\"].append(frac_active.detach().cpu())\n",
    "\n",
    "        return data_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHlOBYDEEnOX"
   },
   "source": [
    "### Training our autoencoder\n",
    "\n",
    "The `optimize` method is given to you. It's the same as it was from your previous model, but with a few notable differences:\n",
    "\n",
    "- We're now training our autoencoder on the hidden state activations $h$ of our model, rather than from the original randomly generated features $x$. We don't generate data directly from the `Model.generate_batch` method, but rather we use this method to generate features then use those features to generate hidden state activations for our model, which we use as the input to our autoencoder.\n",
    "- Our autoencoder loss is the sum of the $L_1$ and $L_2$ losses returned by the `forward` function.\n",
    "- The `optimize` method logs some more interesting data, including the fraction of hidden state activations $h$ which are active, for each instance. It also returns these values as a tensor at the end.\n",
    "\n",
    "A few other notes about this class (not essential, you can skip these):\n",
    "\n",
    "- We use `torch.no_grad` rather than `torch.inference_mode` as our decorators, when we're doing things like normalizing the decoder weights. This is because `torch.no_grad` disables gradient computation *without affecting other aspects of the computational graph*. In contrast `torch.inference_mode` is more thorough, not only disabling gradient computation but also optimizing for inference, skipping some computations and memory storage that are unnecessary when you're not backpropagating.\n",
    "    - Summary: use `torch.inference_mode` when you care about inference speed and you're not directly editing model weights; use `torch.no_grad` when you do want to edit model weights and still backpropagate afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PgY8_Mxvw-r"
   },
   "source": [
    "First, we train our model (which is the thing that will be used to produce the data our autoencoder gets trained on), and visually check that it's learned the pentagon superposition pattern which we're hoping for. To make sure this happens, we'll use a very small feature probability (and we'll have the same importance & feature probability across all features & instances, for simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FWJNyOjvw-r"
   },
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    n_instances = 8,\n",
    "    n_features = 5,\n",
    "    n_hidden = 2,\n",
    ")\n",
    "model = Model(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    feature_probability = 0.01,\n",
    ")\n",
    "model.optimize(steps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZFd5TqXvw-s"
   },
   "source": [
    "Verify we get the pentagon pattern we expect, for all instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIDi9ZiMvw-s"
   },
   "outputs": [],
   "source": [
    "plot_features_in_2d(\n",
    "    model.W,\n",
    "    title = \"Superposition: 5 features represented in 2D space\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ2H3U7mvw-s"
   },
   "source": [
    "Generate a random batch, and verify that the embeddings more or less line up with what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW3F0EWHvw-s"
   },
   "outputs": [],
   "source": [
    "batch = model.generate_batch(250)\n",
    "hidden = einops.einsum(batch, model.W, \"batch_size instances features, instances hidden features -> instances hidden batch_size\")\n",
    "\n",
    "plot_features_in_2d(\n",
    "    hidden,\n",
    "    title = \"Hidden state representation of a random batch of data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF4mUxhMvw-s"
   },
   "source": [
    "Next, we train our autoencoder. The `optimize` function is set up to return a dictionary `data_log` containing data which is useful for visualizing the training process. We'll use this to create an animation of the autoencoder training over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b_Vf7f7vw-s"
   },
   "outputs": [],
   "source": [
    "ae_cfg = AutoEncoderConfig(\n",
    "    n_instances = 8,\n",
    "    n_input_ae = 2,\n",
    "    n_hidden_ae = 5,\n",
    "    l1_coeff = 0.5,\n",
    ")\n",
    "autoencoder = AutoEncoder(ae_cfg)\n",
    "\n",
    "data_log = autoencoder.optimize(\n",
    "    model = model,\n",
    "    steps = 10_000,\n",
    "    log_freq = 200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj8Xx_hBvw-s"
   },
   "outputs": [],
   "source": [
    "# Note - unfortunately this takes a long time to run in Colab (~1 min)\n",
    "\n",
    "plot_features_in_2d(\n",
    "    t.stack(data_log[\"W_enc\"], dim=0),\n",
    "    colors = data_log[\"colors\"],\n",
    "    title = data_log[\"titles\"],\n",
    "    colab = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KztddxA2vw-s"
   },
   "outputs": [],
   "source": [
    "# Now check the points are being reconstructed faithfully by our autoencoder\n",
    "batch = model.generate_batch(250)\n",
    "hidden = einops.einsum(batch, model.W, \"batch_size instances features, instances hidden features -> batch_size instances hidden\")\n",
    "hidden_reconstructed = autoencoder.forward(hidden)[-1]\n",
    "\n",
    "plot_features_in_2d(\n",
    "    einops.rearrange(hidden_reconstructed, \"batch_size instances hidden -> instances hidden batch_size\"),\n",
    "    title = \"Autoencoder's reconstruction of the hidden state\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCOR4Vbevw-s"
   },
   "source": [
    "If you've done this correctly, you should see something like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/like_this_2.png\" width=\"600\">\n",
    "\n",
    "In other words, the autoencoder is generally successful at discovering the model's features, and maybe somtimes it's even lucky enough to learn all 5, but most of the time it learns \"dead features\" which never activate. You can check this by graphing the feature probabilities over training, in the code below. You should find that there are 2 types of features: ones which converge to the expected feature probability of $0.01$, and ones which quickly converge to zero. These latter neurons are **dead neurons**, and they're an annoying problem when it comes to training sparse autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh33p6xYvw-s"
   },
   "outputs": [],
   "source": [
    "frac_active_line_plot(\n",
    "    frac_active = t.stack(data_log[\"frac_active\"], dim=0),\n",
    "    feature_probability = 0.01,\n",
    "    title = \"Probability of autoencoder neurons being active during training\",\n",
    "    width = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR5DArrBvw-s"
   },
   "source": [
    "### Neuron resampling\n",
    "\n",
    "From Anthropic's paper:\n",
    "\n",
    "> Second, we found that over the course of training some neurons cease to activate, even across a large number of datapoints. We found that “resampling” these dead neurons during training gave better results by allowing the model to represent more features for a given autoencoder hidden layer dimension. Our resampling procedure is detailed in [Neuron Resampling](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-resampling), but in brief we periodically check for neurons which have not fired in a significant number of steps and reset the encoder weights on the dead neurons to match data points that the autoencoder does not currently represent well.\n",
    "\n",
    "Your next task is to implement this neuron resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfmYw3Cp2cKS"
   },
   "source": [
    "### Exercise - implement `resample_neurons`\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴🔴⚪\n",
    "Importance: 🔵🔵🔵🔵⚪\n",
    "\n",
    "You should spend up to 20-30 minutes on this exercise.\n",
    "```\n",
    "\n",
    "The process Anthropic describes for resampling autoencoder neurons is pretty involved, so we'll start by implementing a simpler version of it. Specifically, we'll implement the following algorithm:\n",
    "\n",
    "* Find the dead neurons for this instance (i.e. the `neuron` values s.t. `frac_active_in_window[:, instance, neuron]` are all zero).\n",
    "* For each dead neuron `dead_feature_idx` in this instance, you should:\n",
    "    * Generate a new random vector `v` of length `n_input_ae`, and normalize it to have unit length.\n",
    "    * Set the decoder weights `W_dec[inst, dead_feature_idx, :]` to this new vector `v`.\n",
    "    * Set the encoder weights `W_enc[inst, :, dead_feature_idx]` to this new vector `v`.\n",
    "    * Set the encoder biases `W_enc[inst, dead_feature_idx]` to zero.\n",
    "\n",
    "Note, we've provided the arguments `h` and `neuron_resample_scale` to you, but you won't have to use them until later exercises.\n",
    "\n",
    "The test function we've given you will check that your function replaces / zeros the correct weights.\n",
    "\n",
    "Note - you don't have to use the for loop structure here, we've included it as a suggestion to make the function easier to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94bT2nq_2cKS"
   },
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def resample_neurons(\n",
    "    self: AutoEncoder,\n",
    "    h: Float[Tensor, \"batch_size n_instances n_hidden\"],\n",
    "    frac_active_in_window: Float[Tensor, \"window n_instances n_hidden_ae\"],\n",
    "    neuron_resample_scale: float,\n",
    ") -> None:\n",
    "    '''\n",
    "    Resamples neurons that have been dead for 'dead_neuron_window' steps, according to `frac_active`.\n",
    "    '''\n",
    "    # Create an object to store the dead neurons (this will be useful for plotting)\n",
    "    dead_features_mask = t.empty((self.cfg.n_instances, self.cfg.n_hidden_ae), dtype=t.bool, device=self.W_enc.device)\n",
    "\n",
    "    for instance in range(self.cfg.n_instances):\n",
    "\n",
    "        # YOUR CODE HERE - find the dead neurons in this instance, and replace the weights for those neurons\n",
    "        pass\n",
    "\n",
    "    # Return data for visualising the resampling process\n",
    "    colors = [[\"red\" if dead else \"black\" for dead in dead_neuron_mask_inst] for dead_neuron_mask_inst in dead_features_mask]\n",
    "    title = f\"resampling {dead_features_mask.sum()}/{dead_features_mask.numel()} neurons (shown in red)\"\n",
    "    return colors, title\n",
    "\n",
    "\n",
    "tests.test_resample_neurons_simple(resample_neurons)\n",
    "\n",
    "AutoEncoder.resample_neurons = resample_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbPwDCVUvw-s"
   },
   "outputs": [],
   "source": [
    "ae_cfg = AutoEncoderConfig(\n",
    "    n_instances = 8,\n",
    "    n_input_ae = 2,\n",
    "    n_hidden_ae = 5,\n",
    "    l1_coeff = 0.25,\n",
    ")\n",
    "autoencoder = AutoEncoder(ae_cfg)\n",
    "\n",
    "data_log = autoencoder.optimize(\n",
    "    model = model,\n",
    "    steps = 20_000,\n",
    "    neuron_resample_window = 2_500,\n",
    "    dead_neuron_window = 400,\n",
    "    neuron_resample_scale = 0.5,\n",
    "    log_freq = 200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7RaT3Vuvw-s"
   },
   "outputs": [],
   "source": [
    "# Note - unfortunately this takes a long time to run in Colab (~2 mins)\n",
    "\n",
    "plot_features_in_2d(\n",
    "    t.stack(data_log[\"W_enc\"], dim=0),\n",
    "    colors = data_log[\"colors\"],\n",
    "    title = data_log[\"titles\"],\n",
    "    colab = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vq5hbwBXvw-s"
   },
   "outputs": [],
   "source": [
    "frac_active_line_plot(\n",
    "    frac_active = t.stack(data_log[\"frac_active\"], dim=0),\n",
    "    feature_probability = 0.01,\n",
    "    y_max = 0.05,\n",
    "    title = \"Probability of autoencoder neurons being active during training\",\n",
    "    width = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFzRw3C_vw-s"
   },
   "source": [
    "### Exercise - implement `resample_neurons` (the deluxe version)\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴🔴🔴\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 20-40 minutes on this exercise.\n",
    "```\n",
    "\n",
    "This section can be considered optional if you've already implemented the simpler version of `resample_neurons` above. However, if you're interested in a version of it which hues close to [Anthropic's methodology](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-resampling), then you might still be interested in this exercise.\n",
    "\n",
    "The main difference we'll make is in how the resampled values are chosen. Rather than just drawing them randomly from a distribution and normalizing them, we'll be **sampling them with replacement from a set of input activations $h$, with sampling probabilities weighted by the squared $L_2$ loss of the autoencoder on each input**. Intuitively, this will make it more likely that our resampled neurons will represent feature directions that the autoencoder is currently doing a bad job of representing.\n",
    "\n",
    "A bit more guidance on exactly how this resampling works:\n",
    "\n",
    "- For each instance `inst`, you'll be sampling `n_dead` vectors (with replacement) from that instance's batch of data `h[:, inst]` (where `n_dead` is the number of dead neurons for that particular instance).\n",
    "- The sample probabilities will be given by `l2_loss[:, inst].pow(2)`, i.e. the squared $L_2$ loss of the autoencoder on that instance's batch of data.\n",
    "- Your new `W_dec` weights will be these sampled vectors, normalized.\n",
    "- Rather than doing the same thing for `W_enc`, we'll follow Anthropic's methodology and use the same vector but a norm of $\\alpha \\beta$ rather than unit norm, where:\n",
    "    - $\\alpha$ is the mean norm of the encoder weights of alive neurons for that particular instance, or just 1.0 if there are no alive neurons.\n",
    "    - $\\beta$ is a hyperparameter which we've given to you as `neuron_resample_scale` (Anthropic uses 0.2 as a default).\n",
    "\n",
    "A few tips / notes:\n",
    "\n",
    "- Make sure to deal with \"divide by zero\" errors. It's not always as simple as \"add a small number to the denominator to make sure we don't get an error\" - think about what dividing by zero would mean in this instance, and what you should do instead. For example, if your $L_2$ loss is zero for a particular instance, then there's no need to resample anything!\n",
    "- When resampling, we recommend using `torch.distributions.categorical.Categorical` to define a probability distribution, which can then be sampled from using the `sample` method. We've included an example of how to use this function below.\n",
    "- This function should definitely be done by iteration over instances, because it's pretty messy to vectorize! We've given you the template for this. Remember that most tensors you're working with have an `n_instances` dimension - make sure you're indexing into them correctly.\n",
    "\n",
    "Note - the tests here are not exhaustive, because there are many small ways this implementation can differ. However, the model performance is also quite robust to small differences in implementation. For example, if you just always use `1.0` rather than `W_enc_norm_alive_mean`, it'll probably still work fine. If you only reset `W_enc` and completely forget to deal with `W_dec`, it'll probably still work fine (this happened to a friend of mine, totally not something I initially forgot about when I first published these exercises). We encourage you to play around with a few different methods, and see what works well!\n",
    "\n",
    "<details>\n",
    "<summary>Example of using <code>Categorical</code>.</summary>\n",
    "\n",
    "```python\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# Define a prob distn over (0, 1, 2, 3, 4) with probs proportional to (4, 3, 2, 1, 0)\n",
    "values = t.arange(5).flip(0)\n",
    "probs = values.float() / values.sum()\n",
    "distribution = Categorical(probs = probs)\n",
    "\n",
    "# Sample a single value from it\n",
    "distribution.sample()\n",
    "\n",
    "# Sample multiple values with replacement (values will mostly be in the lower end of the range)\n",
    "distribution.sample((10,))\n",
    "```\n",
    "\n",
    "If `probs` is 1D with shape `(D,)`, then `sample((k,))` returns a scalar tensor of shape `(k,)` containing samples from the integer range `0 : D`, and `sample()` will return a single scalar tensor.\n",
    "\n",
    "If `probs` is nD with shape `(*N, D)`, then `sample((k,))` returns a tensor of shape `(*N, k)` and `sample()` will return a tensor of shape `(*N,)` (we treat the first `N` dimensions of the `probs` tensor as batch dimensions).\n",
    "\n",
    "Classic gotcha - make sure to always pass in tuples to the `sample` function, and not pass in an `int` by accident. `sample((k))` and `sample((k,))` are not the same thing!\n",
    "\n",
    "</details>\n",
    "\n",
    "Once you've implemented this resampling method and passed the tests, you can try training your SAE again. Can you see how the resampling process is helping the autoencoder to more efficiently learn under-represented features, relative to the previous version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUzobdILFpqK"
   },
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def resample_neurons(\n",
    "    self: AutoEncoder,\n",
    "    h: Float[Tensor, \"batch_size n_instances n_hidden\"],\n",
    "    frac_active_in_window: Float[Tensor, \"window n_instances n_hidden_ae\"],\n",
    "    neuron_resample_scale: float,\n",
    ") -> None:\n",
    "    '''\n",
    "    Resamples neurons that have been dead for 'dead_feature_window' steps, according to `frac_active`.\n",
    "    '''\n",
    "    l2_loss = self.forward(h)[1]\n",
    "\n",
    "    # Create an object to store the dead neurons (this will be useful for plotting)\n",
    "    dead_features_mask = t.empty((self.cfg.n_instances, self.cfg.n_hidden_ae), dtype=t.bool, device=self.W_enc.device)\n",
    "\n",
    "    for instance in range(self.cfg.n_instances):\n",
    "\n",
    "        # YOUR CODE HERE - find the dead neurons in this instance, and replace the weights for those neurons\n",
    "        pass\n",
    "\n",
    "    # Return data for visualising the resampling process\n",
    "    colors = [[\"red\" if dead else \"black\" for dead in dead_feature_mask_inst] for dead_feature_mask_inst in dead_features_mask]\n",
    "    title = f\"resampling {dead_features_mask.sum()}/{dead_features_mask.numel()} neurons (shown in red)\"\n",
    "    return colors, title\n",
    "\n",
    "\n",
    "tests.test_resample_neurons(resample_neurons)\n",
    "\n",
    "AutoEncoder.resample_neurons = resample_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m2yX1d42cKS"
   },
   "outputs": [],
   "source": [
    "ae_cfg = AutoEncoderConfig(\n",
    "    n_instances = 8,\n",
    "    n_input_ae = 2,\n",
    "    n_hidden_ae = 5,\n",
    "    l1_coeff = 0.25,\n",
    ")\n",
    "autoencoder = AutoEncoder(ae_cfg)\n",
    "\n",
    "data_log = autoencoder.optimize(\n",
    "    model = model,\n",
    "    steps = 20_000,\n",
    "    neuron_resample_window = 2_500,\n",
    "    dead_neuron_window = 400,\n",
    "    neuron_resample_scale = 0.5,\n",
    "    log_freq = 200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFSKIatU5N1O"
   },
   "source": [
    "This code plots the encoder and decoder weights separately, on different rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uKeSR8k4tfp"
   },
   "outputs": [],
   "source": [
    "# We'll be plotting encoder & decoder on the first & second rows\n",
    "titles = [title + \", first row = encoder, second row = decoder\" for title in data_log[\"titles\"]]\n",
    "\n",
    "# Stack encoder and decoder along the n_instances dimension\n",
    "data = t.concat([\n",
    "    t.stack(data_log[\"W_enc\"], dim=0),\n",
    "    t.stack(data_log[\"W_dec\"], dim=0).transpose(-1, -2)\n",
    "], dim=1)\n",
    "\n",
    "plot_features_in_2d(\n",
    "    data,\n",
    "    colors = data_log[\"colors\"],\n",
    "    title = titles,\n",
    "    colab = True,\n",
    "    n_rows = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUCBnI-Q2cKS"
   },
   "outputs": [],
   "source": [
    "frac_active_line_plot(\n",
    "    frac_active = t.stack(data_log[\"frac_active\"], dim=0),\n",
    "    feature_probability = 0.01,\n",
    "    y_max = 0.05,\n",
    "    title = \"Probability of autoencoder neurons being active during training\",\n",
    "    width = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAz12vBEvw-s"
   },
   "source": [
    "### Exercise - overcomplete basis\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴⚪⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 5-10 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Try training your autoencoder with an overcomplete basis: `n_hidden_ae` strictly larger than `n_features`.\n",
    "\n",
    "- Does your model learn the features faster?\n",
    "- How long does it take for all features to be learned by at least one of the autoencoder neurons, and how long before all neurons are either dead or representing exactly one feature?\n",
    "- Can you devise a procedure which kills neurons which are highly correlated with other neurons, and thereby allows the model learn a 1-1 correspondence between neurons and features, which is learned faster than when we used `n_hidden_ae = n_features`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu-qFAGTvw-s"
   },
   "source": [
    "### Exercise - tied weights\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 10-15 minutes on this exercise.\n",
    "```\n",
    "\n",
    "One possible approach you can take is to tie your embedding and unembedding weights together, i.e. have `W_dec = W_enc.T` (for any given instance). Try rewriting your code to remove `W_dec` and replace all instances of `W_dec` with the transposed version of `W_enc`. How do your results change? Why do you think this is, and what do you think the justifications are for tying or not tying the weights?\n",
    "\n",
    "<details>\n",
    "<summary>Answer (what you should see, and why)</summary>\n",
    "\n",
    "You should see that the model is able to learn the features more quickly, maybe without resampling any neurons at all.\n",
    "\n",
    "This is because our toy model is a pretty special case, where all the features have the same importance, are independent, and are evenly spaced around the unit circle, meaning there's no real difference between the notion of encoding and decoding directions (they're both equal to the direction in `model.W`). But this might not always be the case. As an example, consider the case where 2 of our 5 features are highly correlated. We would still want the decoder directions to be the same, because the purpose of the decoder is to reconstruct the original features. But the encoder is **optimized to detect interfering features in superposition**, so it might want to learn a slightly different set of directions (e.g. if features are correlated, or they have different importances).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4OS8JzK2cKT"
   },
   "source": [
    "### Exercise - explore untied weights\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 10-15 minutes on this exercise.\n",
    "```\n",
    "\n",
    "In the previous exercises, our model has learned a uniform solution. That is, all represented features are equally spaced around the unit circle, in the 2D hidden dimension. This guarantees that the encoder and decoder weights will be the same (we can argue this by symmetry). But intuitively, we shouldn't expect this to happen all the time. For an idea of why, read [Neel's public comment document](https://docs.google.com/document/u/0/d/187jfZSbhRjjQaazjYlThBsKp3Q0Pw3VdIHVST9H2dvw/mobilebasic) on the dictionary learning paper. TL;DR - the decoder just tries to minimize reconstruction loss so it should represent the features faithfully, but the encoder has the added job of disentangling features which might be in interference with each other.\n",
    "\n",
    "Below is some code to create a model, and manually alter its weights so that two features have high correlation. What do you think will happen when you train an autoencoder on this model? What will happen to the encoder weights? How about the decoder weights?\n",
    "\n",
    "<details>\n",
    "<summary>Hint - if you want to visualise both the encoder and decoder weights simultaneously, you can use the code in this dropdown.</summary>\n",
    "\n",
    "```python\n",
    "# Create data by concatenating encoder and decoder weights\n",
    "data = t.concat([\n",
    "    t.stack(data_log[\"W_enc\"], dim=0),\n",
    "    t.stack(data_log[\"W_dec\"], dim=0).transpose(-1, -2),\n",
    "], dim=1)\n",
    "\n",
    "# By default, len(colors[i]) = cfg.n_instances, so we duplicate colors to work w/ encoder and decoder weights\n",
    "colors = [x if len(x) == 2 * cfg.n_instances else 2 * x for x in data_log[\"colors\"]]\n",
    "\n",
    "plot_features_in_2d(\n",
    "    data,\n",
    "    colors = colors,\n",
    "    title = data_log[\"titles\"],\n",
    "    colab = True,\n",
    "    n_rows = 2, # first row is encoder, second is decoder\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEJeTuNV2cKT"
   },
   "outputs": [],
   "source": [
    "n_instances = 8\n",
    "n_features = 4\n",
    "n_hidden = 2\n",
    "\n",
    "cfg = Config(\n",
    "    n_instances = n_instances,\n",
    "    n_features = n_features,\n",
    "    n_hidden = n_hidden,\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    cfg = cfg,\n",
    "    device = device,\n",
    "    feature_probability = 0.025,\n",
    ")\n",
    "angles = 2 * t.pi * t.tensor([0.0, 0.25, 0.55, 0.70])\n",
    "angles = angles + t.rand((cfg.n_instances, 1)) # shape [instances features]\n",
    "model.W.data = t.stack([t.cos(angles), t.sin(angles)], dim=1).to(device)\n",
    "\n",
    "plot_features_in_2d(\n",
    "    model.W,\n",
    "    title = \"Superposition: 5 features represented in 2D space (non-uniform)\",\n",
    "    subplot_titles = [f\"Instance #{i}\" for i in range(1, 1+n_instances)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVp8CHER2cKT"
   },
   "outputs": [],
   "source": [
    "ae_cfg = AutoEncoderConfig(\n",
    "    n_instances = n_instances,\n",
    "    n_input_ae = n_hidden,\n",
    "    n_hidden_ae = n_features,\n",
    "    l1_coeff = 0.25,\n",
    ")\n",
    "autoencoder = AutoEncoder(ae_cfg)\n",
    "\n",
    "data_log = autoencoder.optimize(\n",
    "    model = model,\n",
    "    steps = 20_000,\n",
    "    neuron_resample_window = 2_500,\n",
    "    dead_neuron_window = 400,\n",
    "    neuron_resample_scale = 0.5,\n",
    "    log_freq = 200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAIfBTIz2cKT"
   },
   "outputs": [],
   "source": [
    "# We'll be plotting encoder & decoder on the first & second rows\n",
    "titles = [title + \", first row = encoder, second row = decoder\" for title in data_log[\"titles\"]]\n",
    "\n",
    "# Stack encoder and decoder along the n_instances dimension\n",
    "data = t.concat([\n",
    "    t.stack(data_log[\"W_enc\"], dim=0),\n",
    "    t.stack(data_log[\"W_dec\"], dim=0).transpose(-1, -2)\n",
    "], dim=1)\n",
    "\n",
    "plot_features_in_2d(\n",
    "    data,\n",
    "    colors = data_log[\"colors\"],\n",
    "    title = titles,\n",
    "    colab = True,\n",
    "    n_rows = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEIICm1Zvw-t"
   },
   "source": [
    "# (7) Sparse Autoencoders in Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuPwVyL7vw-t"
   },
   "source": [
    "Now that we've got an idea of SAEs in toy models, we'll graduate to some more realistic models.\n",
    "\n",
    "Currently, the open-source infrastructure for training SAEs is very much in development, and we'd love you to help build it out! But for now, we'll take a SAE which was trained on the activations of a GELU-1l model (from Neel Nanda's HookedTransformer library).\n",
    "\n",
    "Note - previously we were training our SAEs on a non-privileged basis, but here we're training on the MLP layer. Most of the basic principles carry over - we'll be using the same forward function in our autoencoder, and the same loss function. Note that the \"hidden data\" that we're using as input to our autoencoder is the **post-GeLU activations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3A8xTDRvw-t"
   },
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "from transformer_lens.utils import (\n",
    "    load_dataset,\n",
    "    tokenize_and_concatenate,\n",
    "    download_file_from_hf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp1NG-yivw-t"
   },
   "source": [
    "First, here's some code to load in autoencoders. Neel trained 2 different autoencoders, denoted with the parameter `version` (they are 25 and 47 on HuggingFace). These have the same architectures & were trained in the same way, but with different random seeds. The reason we do this is as follows: one way of testing how \"real\" a feature is is to see whether it's highly correlated with a feature in a different run. If two different runs find the same feature, that's evidence that the feature is universal rather than just a fluke of the training process. From Logan Riggs' post: *there are many ways to be wrong & only one way to be right*.\n",
    "\n",
    "We'll load in both of these autoencoders into the same `AutoEncoder` instance (as 2 different **instances**).\n",
    "\n",
    "A few notes / pieces of terminology:\n",
    "\n",
    "- Because we're working with an MLP layer, we'll use `d_mlp` to denote the size of the MLP layer - this is equivalent to `n_hidden` or `n_features` in the previous section. We'll keep using `n_hidden_ae` to refer to the autoencoder's hidden dimension.\n",
    "- The **dictionary multiplier** (denoted `dict_mult` below) is the factor by which the autoencoder's hidden dimension is larger than the model's hidden dimension, in other words `n_hidden_ae = dict_mult * d_mlp`.\n",
    "- As mentioned in the previous section, there's now possibility for confusion when we refer to \"neurons\" - so we'll disambiguate by using the term \"autoencoder neurons\" for the hidden values of our autoencoder, vs just \"neurons\" to refer to the hidden values of our MLP layer. We'll still refer to \"neuron resampling\" when we talk about resampling autoencoder neurons, though.\n",
    "- On the flipside, there's less confusion when we refer to \"features\" - since we're working with an LLM rather than a toy data-generating process with features explicitly coded in, when we say \"features\" in this section we're usually referring to the autoencoder's hidden values rather than theoretical \"true features\" of the data-generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGtuxxzevw-t"
   },
   "outputs": [],
   "source": [
    "VERSION_DICT = {\"run1\": 25, \"run2\": 47}\n",
    "\n",
    "def load_autoencoder_from_huggingface(versions: List[str] = [\"run1\", \"run2\"]):\n",
    "    state_dict = {}\n",
    "\n",
    "    for version in versions:\n",
    "        version_id = VERSION_DICT[version]\n",
    "        # Load the data from huggingface (both metadata and state dict)\n",
    "        sae_data: dict = download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version_id}_cfg.json\")\n",
    "        new_state_dict: dict = download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version_id}.pt\", force_is_torch=True)\n",
    "        # Add new state dict to the existing one\n",
    "        for k, v in new_state_dict.items():\n",
    "            state_dict[k] = t.stack([state_dict[k], v]) if k in state_dict else v\n",
    "\n",
    "    # Get data about the model dimensions, and use that to initialize our model (with 2 instances)\n",
    "    d_mlp = sae_data[\"d_mlp\"]\n",
    "    dict_mult = sae_data[\"dict_mult\"]\n",
    "    n_hidden_ae = d_mlp * dict_mult\n",
    "\n",
    "    cfg = AutoEncoderConfig(\n",
    "        n_instances = 2,\n",
    "        n_input_ae = d_mlp,\n",
    "        n_hidden_ae = n_hidden_ae,\n",
    "    )\n",
    "\n",
    "    # Initialize our model, and load in state dict\n",
    "    autoencoder = AutoEncoder(cfg)\n",
    "    autoencoder.load_state_dict(state_dict)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "autoencoder = load_autoencoder_from_huggingface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5AnIslpvw-t"
   },
   "source": [
    "We also load in our model from TransformerLens, and inspect it. The model we'll be working with is 1-layer, with `d_mlp = 2048`. Note that the autoencoder we defined above has a dictionary multiplier of 8, meaning it has 2048 * 8 = 16384 neurons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoG5fOjQvw-t"
   },
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qgOh7kuvw-t"
   },
   "source": [
    "Lastly, we load in a large batch of data which is representative of the data which the autoencoder was trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaKugwwAvw-t"
   },
   "outputs": [],
   "source": [
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "print(\"Tokens shape: \", all_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ryzV2Snvw-t"
   },
   "source": [
    "### Exercise - find the sparsity\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 10-20 minutes on this exercise.\n",
    "```\n",
    "\n",
    "You should fill in the `get_feature_probability` function below, which returns the fraction of time each autoencoder feature fires (as a tensor of shape `(2, n_hidden_ae)` - the latter `2` dimension because we have 2 different instances, and you should return the feature probabilities for each feature in both instances).\n",
    "\n",
    "Most of this should be familiar - in the previous sections you've already written code to calculate the model's hidden activations and measured feature sparsity (when you were writing your code to resample autoencoder neurons). The only new thing here is how the hidden activations are generated. You should:\n",
    "\n",
    "- Run the transformer model on `tokens`, and get the post-GELU activations with shape `(batch_size, seq_len, d_mlp)`.\n",
    "- Rearrange this into shape `(batch_size * seq_len, d_mlp)`, because the activations at each token are a separate input for our autoencoder.\n",
    "- Duplicate this tensor into `(batch_size * seq_len, 2, d_mlp)`, because we have 2 different autoencoder instances.\n",
    "- Run the autoencoder on these activations, finding the activations (and the fraction of times the feature is active, over the `batch_size * seq_len` dimension).\n",
    "\n",
    "<details>\n",
    "<summary>Help - I don't know / remember how to extract internal activations from a <code>HookedTransformer</code> model.</summary>\n",
    "\n",
    "The easiest way is to use an `ActivationCache` object. We can run:\n",
    "\n",
    "```python\n",
    "logits, cache = model.run_with_cache(tokens, names_filter=['blocks.0.mlp.hook_post'])\n",
    "```\n",
    "\n",
    "where `names_filter` is a list of the names of the layers we want to extract activations from (in this case, the post-GELU activations in the first and only MLP layer). We can then get the actual activations by indexing into the `cache` object with the same key we used in the `names_filter` argument.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEOSRbzXvw-t"
   },
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def get_feature_probability(\n",
    "    tokens: Int[Tensor, \"bach seq\"],\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: AutoEncoder,\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    '''\n",
    "    Returns the reconstruction loss of the autoencoder on the given batch of tokens.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euvHpD3OEOUZ"
   },
   "outputs": [],
   "source": [
    "# Get a batch of feature probabilities & average them (so we don't put strain on the GPU)\n",
    "feature_probability = [\n",
    "    get_feature_probability(all_tokens[i:i+50], model, autoencoder)\n",
    "    for i in tqdm(range(0, 1000, 50))\n",
    "]\n",
    "feature_probability = sum(feature_probability) / len(feature_probability)\n",
    "\n",
    "log_freq = (feature_probability + 1e-10).log10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W23gB6wPvw-t"
   },
   "outputs": [],
   "source": [
    "# Visualise sparsities for each instance\n",
    "for i, lf in enumerate(log_freq):\n",
    "    hist(\n",
    "        lf,\n",
    "        title=f\"Instance #{i+1}: Log Frequency of Features\",\n",
    "        labels={\"x\": \"log<sub>10</sub>(freq)\"},\n",
    "        histnorm=\"percent\",\n",
    "        template=\"ggplot2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Wye5byOvw-t"
   },
   "source": [
    "We can see that the features are clearly bimodal. Interestingly, further investigation of the lower-frequency group reveals that almost all these features are meaningless, with very high cosine similarity to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlbNlfS1vw-t"
   },
   "outputs": [],
   "source": [
    "# Get all the rare features\n",
    "is_rare = feature_probability[0] < 1e-4\n",
    "rare_encoder_directions = autoencoder.W_enc[0, :, is_rare]\n",
    "rare_encoder_directions_normalized = rare_encoder_directions / rare_encoder_directions.norm(dim=0, keepdim=True)\n",
    "\n",
    "# Compute their pairwise cosine similarities & sample randomly from this N*N matrix of similarities\n",
    "cos_sims_rare = (rare_encoder_directions_normalized.T @ rare_encoder_directions_normalized).flatten()\n",
    "cos_sims_rare_random_sample = cos_sims_rare[t.randint(0, cos_sims_rare.shape[0], (10000,))]\n",
    "\n",
    "# Plot results\n",
    "hist(\n",
    "    cos_sims_rare_random_sample,\n",
    "    marginal=\"box\",\n",
    "    title=\"Cosine similarities of random rare encoder directions with each other\",\n",
    "    labels={\"x\": \"Cosine sim\"},\n",
    "    histnorm=\"percent\",\n",
    "    template=\"ggplot2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeswsKnVvw-t"
   },
   "source": [
    "In Neel's [Public Comment](https://docs.google.com/document/u/0/d/187jfZSbhRjjQaazjYlThBsKp3Q0Pw3VdIHVST9H2dvw/mobilebasic), he discusses this ultra-low frequency cluster (which was also found by Anthropic, although it was around size 5-10% rather than 60%). He speculates that these might be just a curious artifact of training (the encoder directions formed between different runs seem to be the same, although the decoder directions for these features are all very different!).\n",
    "\n",
    "If you're interested in training autoencoders, you can read his document for more details and helpful tips. We won't be discussing the training of autoencoders on real models in these exercises; we'll just be working with Neel's pretrained autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-14_K0wIvw-t"
   },
   "source": [
    "### Exercise - find the reconstruction loss\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 15-25 minutes on this exercise.\n",
    "```\n",
    "\n",
    "You should fill in the `get_reconstruction_loss` function below, which returns the average reconstruction loss (i.e. $L_2$ loss) of each autoencoder instance on a batch of data, in the form of a list of 2 values.\n",
    "\n",
    "This should be very similar to the previous exercise, except rather than getting the activations & returning their sparsity, you should return the average reconstruction loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX2DZFv1vw-t"
   },
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def get_reconstruction_loss(\n",
    "    tokens: Int[Tensor, \"bach seq\"],\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: AutoEncoder,\n",
    ") -> Tuple[float, float]:\n",
    "    '''\n",
    "    Returns the reconstruction loss of the autoencoder on the given batch of tokens.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "get_reconstruction_loss(all_tokens[:10], model, autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4-u_8OXvw-t"
   },
   "source": [
    "You should find that the reconstruction loss is around 0.016 (and about the same for encoder-B). You can compare this to the average squared $L_2$ norm of the activations to get an idea of how good this is - this value is around 0.11, so we're doing a pretty good job (albeit not perfect) of reconstructing the activations.\n",
    "\n",
    "You can also measure the average cosine similarity between the activations and the reconstructions over neurons. In other words, for each neuron we can store the activations & reconstructed activations over all datapoints, treating these as vectors of length `batch_size * seq_len`, then calculate their cosine similarity, and average them over all neurons. (Note the important distinction here - when calculating cos sim, we're reducing over the batch dimension then averaging these values over the `d_mlp` direction, rather than the other way around.) You should find values of around 88%, showing that on average the autoencoder reconstructs the neuron values pretty faithfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKsp6Y0tvw-u"
   },
   "source": [
    "### Exercise - find the substitution loss\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴⚪⚪\n",
    "Importance: 🔵🔵⚪⚪⚪\n",
    "\n",
    "You should spend up to 15-25 minutes on this exercise.\n",
    "```\n",
    "\n",
    "*Note - if you're less familiar with `TransformerLens` syntax, you might want to skip this exercise. It's not crucial to understanding the rest of the notebook.*\n",
    "\n",
    "You should fill in the `get_substitution_loss` function below, which returns the average cross-entropy loss per token under each of the following 4 conditions:\n",
    "\n",
    "- the clean model (no interventions),\n",
    "- activations are replaced with encoder A's reconstructions,\n",
    "- activations are replaced with encoder B's reconstructions,\n",
    "- activations are zero-ablated.\n",
    "\n",
    "It returns this as a tuple of 4 elements.\n",
    "\n",
    "You can re-use some of your code above, to get the autoencoder's reconstruction. The easiest way to do this is to define a **hook function** which replaces the post-GELU activations with the autoencoder's reconstruction.\n",
    "\n",
    "<details>\n",
    "<summary>Help - I don't know / remember how to use hook functions in <code>HookedTransformer</code> model.</summary>\n",
    "\n",
    "The basic syntax you'll need is:\n",
    "\n",
    "```python\n",
    "def hook_function(activations: Float[Tensor, \"batch seq d_mlp\"], hook: HookPoint):\n",
    "    # Intervene on activations: calculating a tensor of 'new_activations'\n",
    "    ...\n",
    "    return new_activations\n",
    "```\n",
    "\n",
    "You can then run with hooks (and return the loss per token) using:\n",
    "\n",
    "```python\n",
    "loss = model.run_with_hooks(\n",
    "    tokens,\n",
    "    return_type = \"loss\",\n",
    "    loss_per_token = True,\n",
    "    fwd_hooks = [(\"blocks.0.mlp.hook_post\", hook_function)],\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJnp79vRvw-u"
   },
   "source": [
    "Note - it is possible to do all of this in a single forward pass. However, it's probably easiest to break it up into 4 forward passes: one cached run which gives you the clean loss & the post-GELU activations, then three runs where you patch in with encoder A's reconstructions / encoder B's reconstructions / zero respectively. The solutions colab contains examples of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UP6vquLBvw-u"
   },
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def get_substitution_loss(\n",
    "    tokens: Int[Tensor, \"bach seq\"],\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: AutoEncoder,\n",
    ") -> Tuple[Float, Float, Float, Float]:\n",
    "    '''\n",
    "    Returns the substitution loss of the autoencoder on the given batch of tokens.\n",
    "\n",
    "    For efficiency, we do this all in one forward pass, but with 3 copies of the tokens (one for a\n",
    "    clean run, and 2 for each of the autoencoder patching runs).\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRLaCTPuvw-u"
   },
   "source": [
    "Once you've done this, run the code below to find the **score** of an autoencoder. This is defined as 1 minus the fraction of increase in loss their reconstructions cause relative to zero-ablation. The idea here is that a score of zero means the autoencoder's reconstructions are no better than zero-ablation, and a score of 1 means their reconstructions are perfect, not increasing loss at all when we substitute them in. You should find your instances' reconstruction scores are at least 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Yn4usCjvw-u"
   },
   "outputs": [],
   "source": [
    "loss_clean, loss_reconstructed_A, loss_reconstructed_B, loss_zero_ablation = get_substitution_loss(all_tokens[:5], model, autoencoder)\n",
    "\n",
    "score_A = ((loss_zero_ablation - loss_reconstructed_A)/(loss_zero_ablation - loss_clean))\n",
    "score_B = ((loss_zero_ablation - loss_reconstructed_B)/(loss_zero_ablation - loss_clean))\n",
    "\n",
    "print(f\"Reconstruction Score A: {score_A:.2%}\")\n",
    "print(f\"Reconstruction Score B: {score_B:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZMhAzLTvw-u"
   },
   "source": [
    "### Exercise - find highest-activating tokens\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴🔴⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 20-35 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Now we've looked at some high-level stuff, let's get down to some deeper dives: trying to interpret particular neurons.\n",
    "\n",
    "The first thing we should do is find the highest-activating sequences for each feature. This can give us a good starting idea of what this neuron is doing. Note that this shouldn't be the end point of our analysis, because this kind of methodology can suffer from the **interpretability illusion** - the top activating examples from a particular dataset often look like they tell a convincing story, until you try a different dataset and get a totally different story! Ideally you'd want to perform tests like passing particular sequences through the model which you expect will / won't cause your feature to activate, thereby testing your hypotheses. However, this still a useful starting point.\n",
    "\n",
    "The function `highest_activating_tokens` returns a tensor of shape `(k, 2)`, where the `i`-th element of this tensor are the (batch, seq) indices of the `i`-th highest-activating token (i.e. the token on which the `feature_idx`-th neuron in the autoencoder has the largest activations). If also returns a tensor of shape `(k,)` containing these activation values.\n",
    "\n",
    "Note that the function also takes a boolean argument `autoencoder_B`, which is `True` when we're looking at the second autoencoder (i.e. the second instance).\n",
    "\n",
    "Tip - you can make this function a lot more efficient by calculating the activations explicitly, rather than by extracting `acts` from the 5-tuple returned by `autoencoder.forward`. This is because you only need one feature - it's wasteful to calculate all 16384! You can just copy & modify code from the start of the `AutoEncoder.forward` method.\n",
    "\n",
    "You can test your function by running the code in the cell below. Click the toggle below to see what sequences you should be getting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9mTD1ax-5c8"
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary>Spoiler: sequences you should be getting</summary>\n",
    "\n",
    "If you run the function with at least a batch size of ~100, you should observe the following pattern: **the top-activating tokens for feature #7 are consistently pronouns like ` I`, ` you` or ` it`, commonly following either the word ` and` or ` but`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKoMtRCGvw-u"
   },
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def highest_activating_tokens(\n",
    "    tokens: Int[Tensor, \"batch seq\"],\n",
    "    model: HookedTransformer,\n",
    "    autoencoder: AutoEncoder,\n",
    "    feature_idx: int,\n",
    "    autoencoder_B: bool = False,\n",
    "    k: int = 10,\n",
    ") -> Tuple[Int[Tensor, \"k 2\"], Float[Tensor, \"k\"]]:\n",
    "    '''\n",
    "    Returns the indices & values for the highest-activating tokens in the given batch of data.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "def display_top_sequences(top_acts_indices, top_acts_values, tokens):\n",
    "    s = \"\"\n",
    "    for (batch_idx, seq_idx), value in zip(top_acts_indices, top_acts_values):\n",
    "        # Get the sequence as a string (with some padding on either side of our sequence)\n",
    "        seq_start = max(seq_idx - 5, 0)\n",
    "        seq_end = min(seq_idx + 5, all_tokens.shape[1])\n",
    "        seq = \"\"\n",
    "        # Loop over the sequence, adding each token to the string (highlighting the token with the large activations)\n",
    "        for i in range(seq_start, seq_end):\n",
    "            new_str_token = model.to_single_str_token(tokens[batch_idx, i].item()).replace(\"\\n\", \"\\\\n\").replace(\"<|BOS|>\", \"|BOS|\")\n",
    "            if i == seq_idx:\n",
    "                new_str_token = f\"[bold u dark_orange]{new_str_token}[/]\"\n",
    "            seq += new_str_token\n",
    "        # Print the sequence, and the activation value\n",
    "        s += f'Act = {value:.2f}, Seq = \"{seq}\"\\n'\n",
    "\n",
    "    rprint(s)\n",
    "\n",
    "tokens = all_tokens[:200]\n",
    "top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, autoencoder, feature_idx=7, autoencoder_B=False)\n",
    "display_top_sequences(top_acts_indices, top_acts_values, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB4rGGqevw-u"
   },
   "source": [
    "### Exercise - find a feature's logit effect\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 10-20 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Understanding when it fires is one thing, but it's also interesting to see what tokens this feature boosts / suppresses when it fires.\n",
    "\n",
    "Try to find the 10 most boosted / suppressed tokens for the `feature_idx = 7` which is shown above. In other words, when you multiply the decoder weight with the GELU-1l model's MLP output matrix and unembedding matrix, which tokens are most affected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHG3KDK3-7dD"
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary>Hint - names of the weights</summary>\n",
    "\n",
    "You will need the following two weight matrices:\n",
    "\n",
    "```python\n",
    "model.W_out[0] # shape [d_mlp, d_model]\n",
    "model.W_U # shape [d_model, d_vocab]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution (some sample code & what you should find when you run it)</summary>\n",
    "\n",
    "Code to get & print the results:\n",
    "\n",
    "```python\n",
    "W_dec_vector = autoencoder.W_dec[0, 7]\n",
    "\n",
    "W_dec_logits = W_dec_vector @ model.W_out[0] @ model.W_U\n",
    "\n",
    "top_tokens = W_dec_logits.topk(10)\n",
    "bottom_tokens = W_dec_logits.topk(10, largest=False)\n",
    "\n",
    "s = \"Top tokens:\\n\"\n",
    "for token, value in zip(top_tokens.indices, top_tokens.values):\n",
    "    s += f\"({value:.2f}) {model.to_single_str_token(token.item())}\\n\"\n",
    "s += \"\\nBottom tokens:\\n\"\n",
    "for token, value in zip(bottom_tokens.indices, bottom_tokens.values):\n",
    "    s += f\"({value:.2f}) {model.to_single_str_token(token.item())}\\n\"\n",
    "rprint(s)\n",
    "```\n",
    "\n",
    "You should find that the most boosted token is `'ll` - not surprising given this commonly follows `I`, `we` and `it`. It seems likely that this feature is mostly a **bigram feature**. Some other highly boosted tokens also commonly follow these pronouns, e.g. `hope`, `wouldn`, `definitely`.\n",
    "\n",
    "Note - when we say **bigram feature**, we mean features which seem to exist in order to model bigram frequences: *\"this token is X, so next token is more/less likely to be Y\"*. These are uninteresting because they don't require attention heads to move information around, they're just the feature responding to something already in the residual stream & writing directly to the output.\n",
    "\n",
    "However, this feature might not only be modelling bigrams. We saw that it activates most when the pronoun is followed by tokens like `and` or `but`. Possibly this is better described as a **trigram feature**, which is slightly more interesting because it must involve attention heads in some way. Can you find any **[skip-trigram](https://transformer-circuits.pub/2021/framework/index.html#interpretation-as-skip-trigrams) features**?\n",
    "\n",
    "The bottom 10 tokens don't seem very interpretable - this is pretty common (most of the time model components are better thought of as boosting rather than suppressing components - although [not all the time](https://arxiv.org/abs/2310.04625)!)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuTj-kezvw-u"
   },
   "outputs": [],
   "source": [
    "W_dec_vector = autoencoder.W_dec[0, 7]\n",
    "\n",
    "W_dec_logits = W_dec_vector @ model.W_out[0] @ model.W_U\n",
    "\n",
    "top_tokens = W_dec_logits.topk(10)\n",
    "bottom_tokens = W_dec_logits.topk(10, largest=False)\n",
    "\n",
    "s = \"Top tokens:\\n\"\n",
    "for token, value in zip(top_tokens.indices, top_tokens.values):\n",
    "    s += f\"({value:.2f}) {model.to_single_str_token(token.item())}\\n\"\n",
    "s += \"\\nBottom tokens:\\n\"\n",
    "for token, value in zip(bottom_tokens.indices, bottom_tokens.values):\n",
    "    s += f\"({value:.2f}) {model.to_single_str_token(token.item())}\\n\"\n",
    "rprint(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq-OzCbl2cKV"
   },
   "source": [
    "### Exercise - examine this feature in the neuron basis\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴⚪⚪⚪\n",
    "Importance: 🔵🔵🔵⚪⚪\n",
    "\n",
    "You should spend up to 10-20 minutes on this exercise.\n",
    "```\n",
    "\n",
    "An important question which we've not asked yet - **are the features sparse or dense in the neuron basis?** When we say a feature sparse in the neuron basis, we mean that this feature only has exposure to a small number of neurons. In the extreme case, if a feature is literally identical to a neuron, then the corresponding feature vector `W_dec[instance, feature_idx, :]` contains all 0s, except for 1 at a single neuron. On the other hand, if a feature is dense in the neuron basis, this is evidence that our autoencoder is finding features which are genuinely in superposition (or at least polysemantic), and these features wouldn't be found if we just looked at individual neurons in the MLP layer.\n",
    "\n",
    "(Note, we're looking at the decoder weights here, because as we've discussed before, the encoder weights might be pushed apart from each other by the necessity of them having to reduce interference between features, and this might make them artificially dense in the neuron basis if this pushes them away from the direction of any single neuron.)\n",
    "\n",
    "Look at the top neurons which the previously analyzed `instance=0, feature_idx=7` feature is most exposed to. What fraction of this feature's $L_1$ norm is taken up by the top neuron? By the top 3 neurons? Do these neurons activate most strongly on similar tokens to this feature (i.e. pronouns)? Do they boost tokens similar to the ones boosted by this feature (e.g. `'ll` and other tokens that commonly follow pronouns)?\n",
    "\n",
    "<details>\n",
    "<summary>Spoiler</summary>\n",
    "\n",
    "You should find that this feature is very dense in the neuron basis, with less than 1% of its $L_1$ norm being associated with any single neuron.\n",
    "\n",
    "You should also find that a handful of the neurons (but far from all of them) activate on similar tokens to this feature. For example, the top neuron (still with only 0.71% $L_1$ exposure from this feature) has top-activating tokens `[' he', ' she']`, the 4th neuron has `' THEY'` as its second-highest activating token, and the 10th neuron has `['she', 'She']` as its top two. The story is roughly similar when we look at which tokens are most strongly predicted for this feature, with the top neuron & 10th neuron both boosting verbs which commonly follow pronouns (although interestingly, none of the top 10 neurons seem to have `'ll` as one of their top boosted tokens - another sign that this feature has spotted something which isn't just a property of the neuron basis).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fURtlCkc2cKW"
   },
   "outputs": [],
   "source": [
    "l1_norms = autoencoder.W_dec[0, 7, :].abs()\n",
    "l1_norms_as_fraction = l1_norms / l1_norms.sum()\n",
    "\n",
    "top_l1_norms_values, top_l1_norms_indices = l1_norms_as_fraction.topk(10)\n",
    "\n",
    "top_l1_neurons_top_activating_tok_indices = (model.W_E @ model.W_in[0])[:, top_l1_norms_indices].topk(k=3, dim=0).indices\n",
    "top_l1_neurons_top_activating_toks = [model.to_str_tokens(indices) for indices in top_l1_neurons_top_activating_tok_indices.T]\n",
    "\n",
    "top_l1_neurons_top_predicted_tok_indices = (model.W_U.T @ model.W_out[0].T)[:, top_l1_norms_indices].topk(k=3, dim=0).indices\n",
    "top_l1_neurons_top_predicted_toks = [model.to_str_tokens(indices) for indices in top_l1_neurons_top_predicted_tok_indices.T]\n",
    "\n",
    "s = \"[b u]Top neurons by L1 norm of decoder, along with those neurons' top activating & predicted tokens[/]\\n\"\n",
    "for i in range(10):\n",
    "    s += f\"\"\"\n",
    "Neuron alignment (pct of L1) = {top_l1_norms_values[i]:.2%}\n",
    "top activating tokens = {top_l1_neurons_top_activating_toks[i]}\n",
    "top boosted tokens = {top_l1_neurons_top_predicted_toks[i]}\n",
    "\"\"\"\n",
    "\n",
    "rprint(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdXOOjT2vw-v"
   },
   "source": [
    "### Exercise - find some fun features! (optional)\n",
    "\n",
    "```c\n",
    "Difficulty: 🔴🔴🔴🔴⚪\n",
    "Importance: 🔵🔵⚪⚪⚪\n",
    "\n",
    "You should spend up to 45 minutes on this exercise.\n",
    "```\n",
    "\n",
    "There are many more fun features to find in this model. In the first 1000 features for instance A, you should find:\n",
    "\n",
    "- A feature which fires on Django syntax (the Python library), and predicts the string `django` following an open bracket (this is importantly different from just a regular bigram feature, because it requires the context before the left braacket to detect that the library is Django).\n",
    "- A feature which fires on dashes following the word `multi`, and predicts words which commonly follow (e.g. `multi-million` or `multi-purpose`).\n",
    "- A feature which fires on the digit `0` when it's part of a year (particularly when it's the hundreds digit) e.g. in `2012`, and boosts `0` and `1`.\n",
    "\n",
    "<br>\n",
    "\n",
    "Can you find these features, e.g. by passing in prompts and seeing which features activate most?\n",
    "\n",
    "<br>\n",
    "\n",
    "Can you find other metrics which lead you to interpretable prompts? We mentioned earlier that bigram features are less interesting than features which require the existence of attention heads to move information around. What happens when you zero-ablate the attention heads at all probabilities except for those where a token is attending to itself, and look for features which are most affected by this ablation? Are any of these features interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "triKEiX8AxRC"
   },
   "source": [
    "# Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wnf1mYB5DS_"
   },
   "source": [
    "Here are some other papers or blog posts you might want to read, which build on the ideas we discussed in this section.\n",
    "\n",
    "There are also a number of papers here which study individual neurons. So far, we've mainly discussed what Neel refers to as **bottleneck superposition**, when a low-dimensional space is forced to act as a kind of \"storage\" for a higher-dimensional space. This happens in transformers, e.g. with the residual stream as the lower-dimensional space, and the space of all possible features as the (much) higher-dimensional space. We've not considered **neuron superposition**, which is what happens when there are more features represented in neuron activation space than there are neurons.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Toy Models of Superposition\n",
    "\n",
    "* [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf)\n",
    "    * Paper by Redwood, which builds on the ideas we discussed in this section.\n",
    "    * They define a measure called **capacity**, which is the same as what we called **dimensionality** above.\n",
    "    * They study the effect that sparsity and kurtosis of the input distribution has on optimal capacity allocation.\n",
    "* [Softmax Linear Units](https://transformer-circuits.pub/2022/solu/index.html)\n",
    "    * This is a proposed architectural change which appears to increase the number of interpretable MLPs with low performance cost. In particular, it may reduce the instance of superposition.\n",
    "    * TL;DR: SOLU is an activation function $\\vec{x} \\to \\vec{x} * \\operatorname{softmax}(\\vec{x})$ which encourages sparsity in activations in the same way that softmax encourages sparsity (often softmax'ed probability distributions have one probability close to one and the others close to zero). Encouraging activation sparsity might make it harder for neurons to be polysemantic.\n",
    "    * Note that several transformers in the TransformerLens library have been trained with SOLU - see the [model page](https://neelnanda-io.github.io/TransformerLens/model_properties_table.html) for more details.\n",
    "    * You can also read why Anthropic currently feels dictionary learning is more promising than architectural approaches in the dictionary learning paper.\n",
    "\n",
    "<br>\n",
    "\n",
    "### SAEs / Sparse Probing\n",
    "\n",
    "* [The rest of the Anthropic dictionary learning paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html)\n",
    "    * There are many other interesting topics we didn't have time to dive into here, such as automated interpretability, feature motifs, and finite-state automata.\n",
    "    * There's also a [Future Work](https://transformer-circuits.pub/2023/monosemantic-features/index.html#discussion-future-work) section at the end, which readers might find interesting for any project ideas!\n",
    "* [Anthropic SAE Feature Browser](https://transformer-circuits.pub/2023/monosemantic-features/vis/a1.html)\n",
    "    * In this interface, you can browse through all the features learned by Anthropic's sparse autoencoder. You can see when features fire, what tokens they boost / suppress, and how they affect the loss.\n",
    "    * Also see [here](https://transformer-circuits.pub/2023/monosemantic-features/index.html#setup-interface) for a guide to the interface.\n",
    "* [Finding Neurons in a Haystack: Case Studies with Sparse Probing](https://arxiv.org/abs/2305.01610)\n",
    "    * The authors train a set of sparse linear probes on neuron activations to predict the presence of certain input features.\n",
    "    * They manage to find **sparse combinations of neurons which represent many features in superposition**, e.g. a neuron which activates on the bigram phrase \"social security\" but not either word individually (see image below).\n",
    "    * Note that this paper is slightly less relevant now that dictionary learning with SAEs has superceded its methodology - but it still represents a large step forward for the goal of extracting features from superposition in MLPs.\n",
    "    <img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/socialsecurity.png\" width=\"750\">"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jd3LpCav3UXu",
    "0W0zgPVMw0XP",
    "oMUtONps5DS4",
    "fTzBhMGj5DS6",
    "EnaswYse5DS9",
    "UygCcgPP2cKP",
    "2MD88v4Zvw-r",
    "xEIICm1Zvw-t",
    "triKEiX8AxRC"
   ],
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
